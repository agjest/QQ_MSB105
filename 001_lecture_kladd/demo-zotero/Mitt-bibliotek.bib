
@misc{2014,
  title = {Morgan {{Stanley}}: {{ETF Tracking Error Dropping}}},
  shorttitle = {Morgan {{Stanley}}},
  year = {2014},
  month = dec,
  abstract = {ETF investors are reaping the benefits of an industry that is cutting down fund tracking error.},
  file = {/Users/ag/Zotero/storage/LLH7D2XR/11269-morgan-stanley-etf-tracking-error-dropping-morgan-stanley-etf-tracking-error-dropping-.html},
  howpublished = {https://www.etf.com/sections/features/11269-morgan-stanley-etf-tracking-error-dropping-morgan-stanley-etf-tracking-error-dropping-.html},
  journal = {ETF.com},
  language = {en}
}

@book{2015,
  title = {Access to and Preservation of Scientific Information in {{Europe}}: Report on the Implementation of {{Commission Recommendation C}}(2012) 4890 Final.},
  shorttitle = {Access to and Preservation of Scientific Information in {{Europe}}},
  year = {2015},
  publisher = {{Publications Office}},
  address = {{LU}},
  language = {eng}
}

@book{2015a,
  title = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}: {{Report}} on the {{Implementation}} of {{Commission Recommendation C}}(2012) 4890 {{Final}}.},
  shorttitle = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}},
  year = {2015},
  publisher = {{Publications Office}},
  address = {{LU}},
  keywords = {done},
  language = {English}
}

@book{2015b,
  title = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}: {{Report}} on the {{Implementation}} of {{Commission Recommendation C}}(2012) 4890 {{Final}}.},
  shorttitle = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}},
  year = {2015},
  publisher = {{Publications Office}},
  address = {{LU}},
  keywords = {done},
  language = {English}
}

@misc{2019,
  title = {Tracking {{Error Formula}} | {{Step}} by {{Step Calculation}} (with {{Examples}})},
  year = {2019},
  month = mar,
  abstract = {Guide to Tracking Error~Formula. Here we discuss how to calculate tracking error for the portfolio along with examples and downloadable excel template.},
  chapter = {Portfolio Management in Finance},
  file = {/Users/ag/Zotero/storage/YBWR275X/tracking-error-formula.html},
  journal = {WallStreetMojo},
  language = {en-US}
}

@book{AccessPreservationScientific2015,
  title = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}: {{Report}} on the {{Implementation}} of {{Commission Recommendation C}}(2012) 4890 {{Final}}.},
  shorttitle = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}},
  year = {2015},
  publisher = {{Publications Office}},
  address = {{LU}},
  keywords = {done},
  language = {English}
}

@book{AccessPreservationScientific2015a,
  title = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}: {{Report}} on the {{Implementation}} of {{Commission Recommendation C}}(2012) 4890 {{Final}}.},
  shorttitle = {Access to and {{Preservation}} of {{Scientific Information}} in {{Europe}}},
  year = {2015},
  publisher = {{Publications Office}},
  address = {{LU}},
  keywords = {done},
  language = {English}
}

@article{adair2000,
  title = {House Prices and Accessibility: The Testing of {{Relationships}} within the {{Belfast}} Urban Area},
  author = {Adair, A and McGreal, S and Smyth, A and Cooper, J and Ryley, T},
  year = {2000},
  volume = {15},
  pages = {699--716},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Housing studies},
  keywords = {ocp022019_REGION_template}
}

@misc{adam2018,
  title = {My Favourite {{R}} Package for: Summarising Data},
  shorttitle = {My Favourite {{R}} Package For},
  author = {{Adam}},
  year = {2018},
  month = jan,
  abstract = {Hot on the heels of delving into the world of R frequency table tools, it's now time to expand the scope and think about data summary functions in general. One of the first steps analysts sho\ldots},
  file = {/Users/ag/Zotero/storage/36MJQ97Q/my-favourite-r-package-for-summarising-data.html},
  journal = {Dabbling with Data},
  language = {en}
}

@misc{adam2018a,
  title = {My Favourite {{R}} Package for: Summarising Data},
  shorttitle = {My Favourite {{R}} Package For},
  author = {{Adam}},
  year = {2018},
  month = jan,
  abstract = {Hot on the heels of delving into the world of R frequency table tools, it's now time to expand the scope and think about data summary functions in general. One of the first steps analysts sho\ldots},
  file = {/Users/ag/Zotero/storage/2UNFTJQZ/my-favourite-r-package-for-summarising-data.html},
  journal = {Dabbling with Data},
  language = {en}
}

@article{ahlfeldt2011,
  title = {If Alonso Was Right: Modelling Accessibility and Explaining the Residential Land Gradient},
  author = {Ahlfeldt, G},
  year = {2011},
  volume = {51},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Regional Science},
  keywords = {ocp022019_REGION_template},
  number = {2},
  unidentified = {3138-338}
}

@article{alexanas1998,
  title = {Urban Spatial Structure},
  author = {Alex Anas, Richard Arnott and Small, Kenneth A.},
  year = {1998},
  month = sep,
  volume = {36},
  pages = {1416--1464},
  date-added = {2015-10-16 12:40:03 +0000},
  date-modified = {2015-10-16 12:42:06 +0000},
  journal = {Journal of Economic Literature},
  number = {3}
}

@book{allaire2020,
  title = {Rmarkdown: {{Dynamic}} Documents for r},
  author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
  year = {2020}
}

@book{allaire2020a,
  title = {Rmarkdown: {{Dynamic Documents}} for r},
  author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
  year = {2020},
  keywords = {done}
}

@book{allaire2020b,
  title = {Rmarkdown: {{Dynamic Documents}} for r},
  author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
  year = {2020},
  keywords = {done}
}

@book{allaireRmarkdownDynamicDocuments2020,
  title = {Rmarkdown: {{Dynamic Documents}} for r},
  author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
  year = {2020},
  keywords = {done}
}

@book{allaireRmarkdownDynamicDocuments2020a,
  title = {Rmarkdown: {{Dynamic Documents}} for r},
  author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
  year = {2020},
  keywords = {done}
}

@article{almas2012,
  title = {{Norske b\o rshandlede fond : en kvantitativ analyse av fondenes egenskaper}},
  shorttitle = {{Norske b\o rshandlede fond}},
  author = {Alm{\aa}s, P{\aa}l Jonas Brand{\aa}str{\o} and Andersen, Kristian Peder M{\o}rtvedt},
  year = {2012},
  abstract = {Denne masterutredningen handler om b\o rshandlede fond tilbudt av DNB og Handelsbanken. Ved hjelp av regresjonsanalyser og t-tester unders\o ker vi hvor godt fondene f\o lger deres referanseindeks, som er OBX-indeksen. Vi tester ogs\aa{} for hvilken fondstilbyder som f\o lger referanseindeksen best. Videre ser vi p\aa{} hvordan fondsmekanismene i girede b\o rshandlede fond endres ved en \guillemotleft kj\o p-og-hold\guillemotright -strategi, og benytter Monte Carlo-simulering, en volatilitetsmodell og en rentemodell for \aa{} unders\o ke dette.  V\aa re analyser tyder p\aa{} at alle fondene historisk har gitt en lavere daglig avkastningsmultippel enn det som er kommunisert i fondsprospektene. Ved sammenligning av fondstilbydere kommer vi frem til at DNB sitt indeksfond og Handelsbankens Bear-fond kommer n\ae rmest lovet avkastningsmultippel i deres kategori. Bull-fondene fra de to tilbyderne klarer vi ikke \aa{} skille. Ved unders\o kelse av fondsmekanismer finner vi ut at ved h\o yere volatilitet, og lengre tidshorisont, vil fondsverdiene bli redusert over tid. Dette viser ogs\aa{} en empirisk analyse vi gjennomf\o rte p\aa{} datasettet v\aa rt.},
  annotation = {Accepted: 2012-08-21T10:23:49Z},
  file = {/Users/ag/Zotero/storage/WEBADNGJ/Almås og Andersen - 2012 - Norske børshandlede fond  en kvantitativ analyse .pdf;/Users/ag/Zotero/storage/EJX2SUIL/169733.html},
  language = {nob}
}

@book{AmericanEconomicAssociation,
  title = {American {{Economic Association}}}
}

@book{AmericanEconomicAssociationa,
  title = {American {{Economic Association}}}
}

@article{anas1998,
  title = {Urban Spatial Structure},
  author = {Anas, A and Arnott, RA and Small, K},
  year = {1998},
  volume = {36},
  pages = {1426--1464},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Economic Literature},
  keywords = {ocp022019_REGION_template}
}

@book{anselin1988,
  title = {Spatial Econometrics: Methods and Models},
  author = {Anselin, L},
  year = {1988},
  publisher = {{Kluwer}},
  address = {{London}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@article{anselin2002,
  title = {Under the Hood. {{Issues}} in the Specification and Interpretation of Spatial Regression Models},
  author = {Anselin, L},
  year = {2002},
  volume = {27},
  pages = {247--267},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Agricultural Economics},
  keywords = {ocp022019_REGION_template}
}

@incollection{anselin2009,
  title = {Spatial Hedonic Models},
  booktitle = {Ch. 26 in Palgrave Handbook of Econometrics Vol 2},
  author = {Anselin, L and {Lozano-Gracia}, N},
  editor = {TC, Mills and (eds), K Patterson},
  year = {2009},
  publisher = {{Palgrave Macmillan}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@manual{aust2019,
  title = {Citr: '{{RStudio}}' Add-in to Insert Markdown Citations},
  author = {Aust, Frederik},
  year = {2019},
  type = {Manual}
}

@article{ball1977,
  title = {Accessibility and Supply Constraints in the Urban Housing Market},
  author = {Ball, MJ and Kirwan, RM},
  year = {1977},
  volume = {14},
  pages = {11--32},
  date-modified = {2012-02-28 12:44:43 +0000},
  journal = {Urban Studies},
  keywords = {ocp022019_REGION_template}
}

@article{bao2004,
  title = {On the Use of Spline Smoothing in Estimating Hedonic Housing Price Models: Empirical Evidence Using {{Hong Kong}} Data},
  author = {Bao, HXH and Wan, ATK},
  year = {2004},
  volume = {32},
  pages = {487--507},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Real Estate Economics},
  keywords = {ocp022019_REGION_template}
}

@article{barnes2010,
  title = {Publish Your Computer Code: It Is Good Enough},
  shorttitle = {Publish Your Computer Code},
  author = {Barnes, Nick},
  year = {2010},
  month = oct,
  volume = {467},
  pages = {753--753},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/467753a},
  file = {/Users/ag/Zotero/storage/W9T2UN9W/Barnes - 2010 - Publish your computer code it is good enough.pdf},
  journal = {Nature},
  language = {en},
  number = {7317}
}

@article{barnes2010a,
  title = {Publish {{Your Computer Code}}: {{It Is Good Enough}}},
  shorttitle = {Publish {{Your Computer Code}}},
  author = {Barnes, Nick},
  year = {2010},
  month = oct,
  volume = {467},
  pages = {753--753},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/467753a},
  journal = {Nature},
  language = {English},
  number = {7317}
}

@article{barnes2010b,
  title = {Publish {{Your Computer Code}}: {{It Is Good Enough}}},
  shorttitle = {Publish {{Your Computer Code}}},
  author = {Barnes, Nick},
  year = {2010},
  month = oct,
  volume = {467},
  pages = {753--753},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/467753a},
  journal = {Nature},
  language = {English},
  number = {7317}
}

@article{barnesPublishYourComputer2010,
  title = {Publish {{Your Computer Code}}: {{It Is Good Enough}}},
  shorttitle = {Publish {{Your Computer Code}}},
  author = {Barnes, Nick},
  year = {2010},
  month = oct,
  volume = {467},
  pages = {753--753},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/467753a},
  journal = {Nature},
  language = {English},
  number = {7317}
}

@article{barnesPublishYourComputer2010a,
  title = {Publish {{Your Computer Code}}: {{It Is Good Enough}}},
  shorttitle = {Publish {{Your Computer Code}}},
  author = {Barnes, Nick},
  year = {2010},
  month = oct,
  volume = {467},
  pages = {753--753},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/467753a},
  journal = {Nature},
  language = {English},
  number = {7317}
}

@article{baum2007,
  title = {Enhanced Routines for Instrumental Variables/Generalized Method of Moments Estimation and Testing},
  author = {Baum, C. F. and Schaffer, M. E. and Stillman, S.},
  year = {2007},
  volume = {7},
  pages = {465--506},
  journal = {Stata Journal},
  keywords = {CUE,Cumby-Huizinga test,endogeneity,Frisch-Waugh-Lovell theorem,GMM,HAC standard errors,heteroskedasticity,instrumental variables,ivactest,ivendog,ivhettest,ivreg2,ivreset,LIML,overid,overidentifying restrictions,ranktest,RESET,serial correlation,weak instruments},
  number = {4}
}

@article{bechhofer2013,
  title = {Why Linked Data Is Not Enough for Scientists},
  author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
  year = {2013},
  month = feb,
  volume = {29},
  pages = {599--611},
  issn = {0167739X},
  doi = {10.1016/j.future.2011.08.004},
  file = {/Users/ag/Zotero/storage/K8CPQ95S/Bechhofer et al. - 2013 - Why linked data is not enough for scientists.pdf},
  journal = {Future Generation Computer Systems},
  language = {en},
  number = {2}
}

@article{bechhofer2013a,
  title = {Why {{Linked Data Is Not Enough}} for {{Scientists}}},
  author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
  year = {2013},
  month = feb,
  volume = {29},
  pages = {599--611},
  issn = {0167739X},
  doi = {10.1016/j.future.2011.08.004},
  journal = {Future Generation Computer Systems},
  keywords = {done},
  language = {English},
  number = {2}
}

@article{bechhofer2013b,
  title = {Why {{Linked Data Is Not Enough}} for {{Scientists}}},
  author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
  year = {2013},
  month = feb,
  volume = {29},
  pages = {599--611},
  issn = {0167739X},
  doi = {10.1016/j.future.2011.08.004},
  journal = {Future Generation Computer Systems},
  keywords = {done},
  language = {English},
  number = {2}
}

@article{bechhoferWhyLinkedData2013,
  title = {Why {{Linked Data Is Not Enough}} for {{Scientists}}},
  author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
  year = {2013},
  month = feb,
  volume = {29},
  pages = {599--611},
  issn = {0167739X},
  doi = {10.1016/j.future.2011.08.004},
  journal = {Future Generation Computer Systems},
  keywords = {done},
  language = {English},
  number = {2}
}

@article{bechhoferWhyLinkedData2013a,
  title = {Why {{Linked Data Is Not Enough}} for {{Scientists}}},
  author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
  year = {2013},
  month = feb,
  volume = {29},
  pages = {599--611},
  issn = {0167739X},
  doi = {10.1016/j.future.2011.08.004},
  journal = {Future Generation Computer Systems},
  keywords = {done},
  language = {English},
  number = {2}
}

@article{bianco2019,
  title = {Understanding Energy Consumption and Carbon Emissions in {{Europe}}: {{A}} Focus on Inequality Issues},
  shorttitle = {Understanding Energy Consumption and Carbon Emissions in {{Europe}}},
  author = {Bianco, Vincenzo and Cascetta, Furio and Marino, Alfonso and Nardini, Sergio},
  year = {2019},
  month = mar,
  volume = {170},
  pages = {120--130},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2018.12.120},
  abstract = {The present research proposes an analysis on the inequality of the consumption of electricity, different typologies of primary energy, namely natural gas, coal and oil, and carbon emissions in the period 2008 -2016 within European Union. A decomposition in within and between country groups on the basis of their GDP per capita is also developed, in order to identify the main contributions to the inequality. Furthermore, carbon emissions are also decomposed according to the Kaya identity with the aim to assess which are the main sources of inequality. The analysis shows that the principal source of inequality is represented by the differences in GDP, especially for the energy consumption; whereas carbon emissions evidence a stable level of inequality during the period of analysis. (C) 2018 Elsevier Ltd. All rights reserved.},
  annotation = {WOS:000460845700013},
  journal = {Energy},
  keywords = {Carbon emissions,co2 emissions,countries,decomposition,efficiency,electricity consumption,Energy consumption,eu,growth,Inequality,intensities,Kaya   identity,policy,Theil index},
  language = {English}
}

@article{bivand1984,
  title = {Regression Modelling with Spatial Dependence: {{An}} Application of Some Class Selection and Estimation Methods},
  author = {Bivand, R},
  year = {1984},
  volume = {16},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Geographical Analysis},
  keywords = {ocp022019_REGION_template},
  number = {1}
}

@book{bivand2008,
  title = {Applied Spatial Data Analysis with {{R}}},
  author = {Bivand, RS and Pebesma, EJ and {G{\'o}mez-Rubio}, V},
  year = {2008},
  publisher = {{Springer}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@misc{bivand2017,
  title = {Spgwr: {{Geographically Weighted Regression}}},
  shorttitle = {Spgwr},
  author = {Bivand, Roger and Yu, Danlin and Nakaya, Tomoki and {Garcia-Lopez}, Miquel-Angel},
  year = {2017},
  month = oct,
  abstract = {Functions for computing geographically weighted regressions are provided, based on work by Chris Brunsdon, Martin Charlton and Stewart Fotheringham.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {Spatial}
}

@book{bodie2014,
  title = {Investments},
  author = {Bodie, Zvi},
  year = {2014},
  edition = {10th global ed.},
  publisher = {{McGraw-Hill Education}},
  address = {{Berkshire}},
  collaborator = {Marcus, Alan J. and Kane, Alex},
  isbn = {978-0-07-716114-9},
  keywords = {Economics,Finance,finansiering,finansmarked,finansøkonomi,investering,Investering,Investeringer,Investeringsformer,Investment,ledelse,Portefølje,porteføljer,portfolio,risikoanalyse,styring,Varer : Investering : Bank- og pengevesen,Verdipapirer : Investering : Bank- og pengevesen},
  language = {eng},
  lccn = {336.767 BOD, 332.6 Bod, 332.6 B, 332.632 BOD, 658.152 Bo, 336.767 Bod, q 332.6 Bod, 332.63 Bod, 332.632 Bod, 336.767 B63i/10.utg.}
}

@techreport{bollen2015,
  title = {Social, {{Behavioral}}, and {{Economic Sciences Perspectives}} on {{Robust}} and {{Reliable Science}}},
  author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
  year = {2015},
  institution = {{NSF}},
  keywords = {done},
  number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences}
}

@techreport{bollen2015a,
  title = {Social, {{Behavioral}}, and {{Economic Sciences Perspectives}} on {{Robust}} and {{Reliable Science}}},
  author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
  year = {2015},
  institution = {{NSF}},
  keywords = {done},
  number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences}
}

@techreport{bollenSocialBehavioralEconomic2015,
  title = {Social, {{Behavioral}}, and {{Economic Sciences Perspectives}} on {{Robust}} and {{Reliable Science}}},
  author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
  year = {2015},
  institution = {{NSF}},
  keywords = {done},
  number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences}
}

@techreport{bollenSocialBehavioralEconomic2015a,
  title = {Social, {{Behavioral}}, and {{Economic Sciences Perspectives}} on {{Robust}} and {{Reliable Science}}},
  author = {Bollen, Kenneth and Cacioppo, John T. and Krosnick, Jon A. and Olds, James L. and Kaplan, Robert M.},
  year = {2015},
  institution = {{NSF}},
  keywords = {done},
  number = {Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences}
}

@inproceedings{brase2009,
  title = {{{DataCite}} - {{A Global Registration Agency}} for {{Research Data}}},
  booktitle = {2009 {{Fourth International Conference}} on {{Cooperation}} and {{Promotion}} of {{Information Resources}} in {{Science}} and {{Technology}}},
  author = {Brase, Jan},
  year = {2009},
  month = nov,
  pages = {257--261},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/COINFO.2009.66},
  file = {/Users/ag/Zotero/storage/SHA7R8YI/Brase - 2009 - DataCite - A Global Registration Agency for Resear.pdf},
  isbn = {978-0-7695-3898-3}
}

@inproceedings{brase2009a,
  title = {{{DataCite}} - {{A Global Registration Agency}} for {{Research Data}}},
  booktitle = {2009 {{Fourth International Conference}} on {{Cooperation}} and {{Promotion}} of {{Information Resources}} in {{Science}} and {{Technology}}},
  author = {Brase, Jan},
  year = {2009},
  month = nov,
  pages = {257--261},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/COINFO.2009.66},
  isbn = {978-0-7695-3898-3},
  keywords = {done}
}

@inproceedings{brase2009b,
  title = {{{DataCite}} - {{A Global Registration Agency}} for {{Research Data}}},
  booktitle = {2009 {{Fourth International Conference}} on {{Cooperation}} and {{Promotion}} of {{Information Resources}} in {{Science}} and {{Technology}}},
  author = {Brase, Jan},
  year = {2009},
  month = nov,
  pages = {257--261},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/COINFO.2009.66},
  isbn = {978-0-7695-3898-3},
  keywords = {done}
}

@inproceedings{braseDataCiteGlobalRegistration2009,
  title = {{{DataCite}} - {{A Global Registration Agency}} for {{Research Data}}},
  booktitle = {2009 {{Fourth International Conference}} on {{Cooperation}} and {{Promotion}} of {{Information Resources}} in {{Science}} and {{Technology}}},
  author = {Brase, Jan},
  year = {2009},
  month = nov,
  pages = {257--261},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/COINFO.2009.66},
  isbn = {978-0-7695-3898-3},
  keywords = {done}
}

@inproceedings{braseDataCiteGlobalRegistration2009a,
  title = {{{DataCite}} - {{A Global Registration Agency}} for {{Research Data}}},
  booktitle = {2009 {{Fourth International Conference}} on {{Cooperation}} and {{Promotion}} of {{Information Resources}} in {{Science}} and {{Technology}}},
  author = {Brase, Jan},
  year = {2009},
  month = nov,
  pages = {257--261},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/COINFO.2009.66},
  isbn = {978-0-7695-3898-3},
  keywords = {done}
}

@misc{breian,
  title = {{Eksperter bekymret: To bekreftede tilfeller av mystisk soppsykdom i Norge}},
  shorttitle = {{Eksperter bekymret}},
  author = {Breian, {\AA}shild},
  abstract = {Den mystiske soppinfeksjonen Candida auris er allerede p\aa vist i Norge. Ekspert innr\o mmer at det kan skyldes flaks at ikke flere er blitt syke.},
  file = {/Users/ag/Zotero/storage/Z9N4CUGV/Eksperter-bekymret-To-bekreftede-tilfeller-av-mystisk-soppinfeksjon-i-Norge.html},
  howpublished = {https://www.aftenposten.no/article/ap-zGOK8b.html},
  journal = {Aftenposten},
  language = {nb-NO}
}

@incollection{buckheit1995,
  title = {{{WaveLab}} and {{Reproducible Research}}},
  booktitle = {Wavelets and {{Statistics}}},
  author = {Buckheit, Jonathan B. and Donoho, David L.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
  year = {1995},
  volume = {103},
  pages = {55--81},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2544-7_5},
  abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
  isbn = {978-0-387-94564-4 978-1-4612-2544-7},
  keywords = {done},
  language = {English}
}

@incollection{buckheit1995a,
  title = {{{WaveLab}} and {{Reproducible Research}}},
  booktitle = {Wavelets and {{Statistics}}},
  author = {Buckheit, Jonathan B. and Donoho, David L.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
  year = {1995},
  volume = {103},
  pages = {55--81},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2544-7_5},
  abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
  isbn = {978-0-387-94564-4 978-1-4612-2544-7},
  keywords = {done},
  language = {English}
}

@incollection{buckheitWaveLabReproducibleResearch1995,
  title = {{{WaveLab}} and {{Reproducible Research}}},
  booktitle = {Wavelets and {{Statistics}}},
  author = {Buckheit, Jonathan B. and Donoho, David L.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
  year = {1995},
  volume = {103},
  pages = {55--81},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2544-7_5},
  abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
  isbn = {978-0-387-94564-4 978-1-4612-2544-7},
  keywords = {done},
  language = {English}
}

@incollection{buckheitWaveLabReproducibleResearch1995a,
  title = {{{WaveLab}} and {{Reproducible Research}}},
  booktitle = {Wavelets and {{Statistics}}},
  author = {Buckheit, Jonathan B. and Donoho, David L.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S. and Antoniadis, Anestis and Oppenheim, Georges},
  year = {1995},
  volume = {103},
  pages = {55--81},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-2544-7_5},
  abstract = {WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
  isbn = {978-0-387-94564-4 978-1-4612-2544-7},
  keywords = {done},
  language = {English}
}

@misc{centerforhistoryandnewmedia,
  title = {Rask Innf\o ring},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide}
}

@article{chakravarty2019,
  title = {Inequality and Welfare: {{Some}} Axiomatic Characterizations},
  shorttitle = {Inequality and Welfare},
  author = {Chakravarty, Satya R. and Chattopadhyay, Nachiketa and Qingbin, Liu},
  year = {2019},
  month = jun,
  volume = {15},
  pages = {153--168},
  issn = {1742-7355},
  doi = {10.1111/ijet.12169},
  abstract = {We characterize social welfare functions, which, under ceteris paribus assumptions, rank income distributions in exactly the same way as the negative of the Atkinson, Kolm-Pollak, and Theil mean logarithmic deviation indices of inequality of income distributions. While the first two characterizations rely on a general social welfare function, the third employs the symmetric utilitarian structure of the welfare function. Implications of several subsets of the set of axioms considered in the paper are also investigated.},
  annotation = {WOS:000467463100002},
  journal = {International Journal of Economic Theory},
  keywords = {Atkinson index,characterization,components,efficiency,equity,gini indexes,inequality,Kolm-Pollak index,polarization,ranking,social   welfare,social-welfare,Theil index},
  language = {English},
  number = {2}
}

@article{chelley-steeley2011,
  title = {Intraday Patterns in {{London}} Listed {{Exchange Traded Funds}}},
  author = {{Chelley-Steeley}, Patricia and Park, Keebong},
  year = {2011},
  month = oct,
  volume = {20},
  pages = {244--251},
  issn = {1057-5219},
  doi = {10.1016/j.irfa.2011.05.001},
  abstract = {In this paper we examine the intraday trading patterns of Exchange Traded Funds (ETFs) listed on the London Stock Exchange. ETFs have been shown to be characterised by much lower bid\textendash ask spread costs and by lower levels of information asymmetry than individual securities. One possible explanation for intraday trading patterns is that concentration of trading arises at the start of the trading day because informed traders have private information that quickly diminishes in value as trading progresses. Since ETFs have lower trading costs and lower levels of information asymmetry we would expect these securities to display less pronounced intraday patterns than individual securities. We fail to find that ETFs are characterised by concentrated trading bouts during the day and therefore find support for the argument that information asymmetry is the cause of intraday volume patterns in stock markets. We find that ETF bid\textendash ask spreads and volatility are elevated at the open but not at the close. This lends support to the ``accumulation of information'' explanation that sees high spreads and volatility at the open as a consequence of information accumulating during a market closure and impacting on the market when it next opens.},
  file = {/Users/ag/Zotero/storage/C5I9XGJJ/S1057521911000482.html},
  journal = {International Review of Financial Analysis},
  keywords = {Intraday trading patterns,London,Spread},
  language = {en},
  number = {5}
}

@article{chin2003,
  title = {A Critical Review of Literature on the Hedonic Price Model},
  author = {Chin, TL and Chau, KW},
  year = {2003},
  volume = {27},
  pages = {145--165},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {International Journal for Housing Science and Its Applications},
  number = {2}
}

@article{collaboration2015,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Collaboration, Open Science},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@article{collaboration2015a,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Collaboration, Open Science},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@article{collaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Collaboration, Open Science},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@article{collaborationEstimatingReproducibilityPsychological2015a,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Collaboration, Open Science},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@misc{conyers,
  title = {Learn {{Git}} in 3 {{Hours}}},
  author = {Conyers, Ross},
  abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...},
  file = {/Users/ag/Zotero/storage/MHMHVXU5/9781789348231-video2_10.html},
  howpublished = {https://www.safaribooksonline.com/videos/learn-git-in/9781789348231/9781789348231-video1\_2},
  journal = {O'Reilly | Safari}
}

@book{conyersa,
  title = {Learn {{Git}} in 3 {{Hours}}},
  author = {Conyers, Ross},
  abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...}
}

@book{conyersb,
  title = {Learn {{Git}} in 3 {{Hours}}},
  author = {Conyers, Ross},
  abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...}
}

@book{conyersLearnGitHours,
  title = {Learn {{Git}} in 3 {{Hours}}},
  author = {Conyers, Ross},
  abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...}
}

@book{conyersLearnGitHoursa,
  title = {Learn {{Git}} in 3 {{Hours}}},
  author = {Conyers, Ross},
  abstract = {Build powerful and effective projects using Git Version Control Systems About This Video Learn how to create, contribute to, and collaborate on software projects using Git Understand its fundamental features,...}
}

@article{coulson1992,
  title = {Semiparametric Estimates of the Marginal Price of Floorspace},
  author = {Coulson, NE},
  year = {1992},
  volume = {5},
  pages = {73--83},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Real Estate Finance and Economics},
  keywords = {ocp022019_REGION_template},
  number = {1}
}

@techreport{cowell2003,
  title = {Theil, {{Inequality}} and the {{Structure}} of {{Income Distribution}}},
  author = {Cowell, Frank A.},
  year = {2003},
  month = may,
  institution = {{Suntory and Toyota International Centres for Economics and Related Disciplines, LSE}},
  abstract = {This paper documents the impact of Argentina's recent economic crises on different aspects of poverty, with a special focus on the economic collapse of 2002. We discuss the methodology of poverty measurement in Argentina and we use a simple rule to compensate for the lack of regional poverty figures until 2001, providing consistent series of urban poverty estimates at the national and regional levels. We then present series of short term dynamics of poverty, decomposing the changes in every period of time with panel data. Finally, we analyse the determinants of poverty, with a focus on accounting for observed differences in income (and thereby poverty) between October 2001 and May 2002. Among other conclusions, we find in our decomposition analysis that households without the means to diversify their income sources suffered more than others from the crisis of 2002.},
  file = {/Users/ag/Zotero/storage/LPUDPKE2/Cowell - 2003 - Theil, Inequality and the Structure of Income Dist.pdf;/Users/ag/Zotero/storage/ZAXMYNRJ/67.html},
  keywords = {homotheticity,independence,inequality,Theil,translatability.},
  language = {en},
  number = {67}
}

@book{DearColleagueLetter,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@book{DearColleagueLettera,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@book{DearColleagueLetterb,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@book{DearColleagueLetterc,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@article{dewald1986,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  shorttitle = {Replication in {{Empirical Economics}}},
  author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
  year = {1986},
  volume = {76},
  pages = {587--603},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
  journal = {The American Economic Review},
  number = {4}
}

@article{dewald1986a,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  shorttitle = {Replication in {{Empirical Economics}}},
  author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
  year = {1986},
  volume = {76},
  pages = {587--603},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
  journal = {The American Economic Review},
  keywords = {done},
  number = {4}
}

@article{dewald1986b,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  shorttitle = {Replication in {{Empirical Economics}}},
  author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
  year = {1986},
  volume = {76},
  pages = {587--603},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
  journal = {The American Economic Review},
  keywords = {done},
  number = {4}
}

@article{dewald2020,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
  year = {2020},
  pages = {18},
  file = {/Users/ag/Zotero/storage/B5HSUWH5/Dewald et al. - 2020 - Replication in Empirical Economics The Journal of.pdf},
  language = {en}
}

@article{dewald2020a,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
  year = {2020},
  pages = {18},
  keywords = {done},
  language = {English}
}

@article{dewald2020b,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
  year = {2020},
  pages = {18},
  keywords = {done},
  language = {English}
}

@article{dewaldReplicationEmpiricalEconomics1986,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  shorttitle = {Replication in {{Empirical Economics}}},
  author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
  year = {1986},
  volume = {76},
  pages = {587--603},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
  journal = {The American Economic Review},
  keywords = {done},
  number = {4}
}

@article{dewaldReplicationEmpiricalEconomics1986a,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  shorttitle = {Replication in {{Empirical Economics}}},
  author = {Dewald, William G. and Thursby, Jerry G. and Anderson, Richard G.},
  year = {1986},
  volume = {76},
  pages = {587--603},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  abstract = {This paper examines the role of replication in empirical economic research. It presents the findings of a two-year study that collected programs and data from authors and attempted to replicate their published results. Our research provides new and important information about the extent and causes of failures to replicate published results in economics. Our findings suggest that inadvertent errors in published empirical articles are a commonplace rather thana rare occurrence.},
  journal = {The American Economic Review},
  keywords = {done},
  number = {4}
}

@article{dewaldReplicationEmpiricalEconomics2020,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
  year = {2020},
  pages = {18},
  keywords = {done},
  language = {English}
}

@article{dewaldReplicationEmpiricalEconomics2020a,
  title = {Replication in {{Empirical Economics}}: {{The Journal}} of {{Money}}, {{Credit}} and {{Banking Project}}},
  author = {Dewald, William G and Thursby, Jerry G and Anderson, Richard G},
  year = {2020},
  pages = {18},
  keywords = {done},
  language = {English}
}

@article{donofrio2019,
  title = {Infrastructures and {{Income Inequality}}: {{The Case}} of {{Italian Provinces}}},
  shorttitle = {Infrastructures and {{Income Inequality}}},
  author = {D'Onofrio, Alexandra and Giordani, Paolo E.},
  year = {2019},
  month = apr,
  volume = {35},
  pages = {27--54},
  issn = {1120-9496},
  doi = {10.1429/93306},
  abstract = {This paper studies the relation between infrastructure endowment and income inequality across Italian provinces over the period 2001-2015. Using data from the Ministero dell'Economia e Finanza (MEF), we construct measures of income inequality for each province (Gini index and Theil index). The stock of infrastructures by province is measured by a set of indicators of physical endowments built by the Istituto Tagliacarne. The empirical analysis confirms the existence of a negative effect of the total infrastructure endowment on income inequality. This effect is mainly driven by transport and energy infrastructures. In contrast, other sub-components of infrastructure endowment, such as technological and banking infrastructures as well as cultural/entertainment networks, accentuate income inequality. We then show that infrastructures have a positive effect on the incomes of both the relatively poor classes (20\% poorest class), the median and the richest class (10\% richest), while they have no effect on the poorest 10\% of the population. Finally, a north/south analysis of Italy suggests that the infrastructure deficit of the southern provinces might play a role in explaining their higher income inequality compared to the northern provinces.},
  annotation = {WOS:000468818600002},
  journal = {Politica Economica},
  keywords = {growth,income distribution,infrastructure endowment,Italian provinces,panel-data,public-goods},
  language = {English},
  number = {1}
}

@article{dubin1992,
  title = {Spatial Autocorrelation and Neighbourhood Quality},
  author = {Dubin, RA},
  year = {1992},
  volume = {22},
  pages = {433--452},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Regional Science and Urban Economics, Vol},
  keywords = {ocp022019_REGION_template},
  number = {3}
}

@article{durand2011,
  title = {Fear and the {{Fama}}-{{French Factors}}},
  author = {Durand, Robert B. and Lim, Dominic and Zumwalt, J. Kenton},
  year = {2011},
  volume = {40},
  pages = {409--426},
  issn = {0046-3892},
  abstract = {Investors' expectations of market volatility, captured by the VIX (the Chicago Board Options Exchange's volatility index, also known as the "investor fear gauge"), affects the expected returns of US equities. Changes in the VIX drive variations in the expected returns of the factors included in the Fama and French three-factor model augmented with a momentum factor. The market risk premium (R m -R f ) and the value premium (HML) are especially sensitive to changes in the VIX. An increase in expected volatility is associated with flights to quality and increases in estimated required returns.},
  journal = {Financial Management},
  number = {2}
}

@article{eliasson2001,
  title = {Transport and {{Location Effects}} of {{Road Pricing}}: {{A Simulation Approach}}},
  shorttitle = {Transport and {{Location Effects}} of {{Road Pricing}}},
  author = {Eliasson, Jonas and Mattsson, Lars-G{\"o}ran},
  year = {2001},
  volume = {35},
  pages = {417--456},
  issn = {0022-5258},
  abstract = {The effects of road pricing on transport and location patterns have been much discussed. However, it is unclear how large the effects are, and whether relocation of households, workplaces and shops will counteract or amplify the effects on transport. In this paper a model of a generic symmetric city is developed. The model is used to investigate the effects of road pricing in the form of congestion pricing and a toll ring. The results indicate that the impacts on location are small compared to the impacts on traffic volumes, modal split, and trip distances. The different effects of congestion pricing and toll rings at different positions are considered.},
  file = {/Users/ag/Zotero/storage/7DDQ96ZF/Eliasson and Mattsson - 2001 - Transport and Location Effects of Road Pricing A .pdf},
  journal = {Journal of Transport Economics and Policy},
  number = {3}
}

@article{ezekiel1933,
  title = {Some Considerations on the Analysis of the Prices of Competing or Substitute Commodities},
  author = {Ezekiel, Mordecai},
  year = {1933},
  volume = {1},
  pages = {172--180},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  number = {2}
}

@article{ezekiel1933a,
  title = {Some {{Considerations}} on the {{Analysis}} of the {{Prices}} of {{Competing}} or {{Substitute Commodities}}},
  author = {Ezekiel, Mordecai},
  year = {1933},
  volume = {1},
  pages = {172--180},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {2}
}

@article{ezekiel1933b,
  title = {Some {{Considerations}} on the {{Analysis}} of the {{Prices}} of {{Competing}} or {{Substitute Commodities}}},
  author = {Ezekiel, Mordecai},
  year = {1933},
  volume = {1},
  pages = {172--180},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {2}
}

@article{ezekielConsiderationsAnalysisPrices1933,
  title = {Some {{Considerations}} on the {{Analysis}} of the {{Prices}} of {{Competing}} or {{Substitute Commodities}}},
  author = {Ezekiel, Mordecai},
  year = {1933},
  volume = {1},
  pages = {172--180},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {2}
}

@article{ezekielConsiderationsAnalysisPrices1933a,
  title = {Some {{Considerations}} on the {{Analysis}} of the {{Prices}} of {{Competing}} or {{Substitute Commodities}}},
  author = {Ezekiel, Mordecai},
  year = {1933},
  volume = {1},
  pages = {172--180},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {2}
}

@article{fama1991,
  title = {Efficient {{Capital Markets}}: {{II}}},
  shorttitle = {Efficient {{Capital Markets}}},
  author = {Fama, Eugene F.},
  year = {1991},
  volume = {46},
  pages = {1575--1617},
  issn = {0022-1082},
  doi = {10.2307/2328565},
  file = {/Users/ag/Zotero/storage/Y2ZKBBD5/Fama - 1991 - Efficient Capital Markets II.pdf},
  journal = {The Journal of Finance},
  number = {5}
}

@article{farber2013,
  title = {The Social Interaction Potential of Metropolitan Regions: A Time-Geographic Measurement Approach Using Joint Accessibility},
  author = {Farber, S and Neutens, T and Miller, H J and Li, X},
  year = {2013},
  pages = {483--504},
  date-added = {2020-01-09 15:04:44 +0100},
  date-modified = {2020-01-09 15:06:20 +0100},
  journal = {Annals of the Association of Americam Geographers},
  keywords = {ocp022019_REGION_template}
}

@techreport{florax2003,
  title = {Misspecification in Linear Spatial Regression Models},
  author = {Florax, RJGM and Nijkamp, P},
  year = {2003},
  institution = {{Tinbergen Institute Discussion Paper, (TI 2003-081/3)}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@article{fomel2009,
  title = {Guest {{Editors}}' {{Introduction}}: {{Reproducible Research}}},
  author = {Fomel, S. and Claerbout, J. F.},
  year = {2009},
  volume = {11},
  pages = {5--7},
  journal = {Computing in Science Engineering},
  keywords = {done},
  number = {1}
}

@article{fomel2009a,
  title = {Guest {{Editors}}' {{Introduction}}: {{Reproducible Research}}},
  author = {Fomel, S. and Claerbout, J. F.},
  year = {2009},
  volume = {11},
  pages = {5--7},
  journal = {Computing in Science Engineering},
  keywords = {done},
  number = {1}
}

@article{fomelGuestEditorsIntroduction2009,
  title = {Guest {{Editors}}' {{Introduction}}: {{Reproducible Research}}},
  author = {Fomel, S. and Claerbout, J. F.},
  year = {2009},
  volume = {11},
  pages = {5--7},
  journal = {Computing in Science Engineering},
  keywords = {done},
  number = {1}
}

@article{fomelGuestEditorsIntroduction2009a,
  title = {Guest {{Editors}}' {{Introduction}}: {{Reproducible Research}}},
  author = {Fomel, S. and Claerbout, J. F.},
  year = {2009},
  volume = {11},
  pages = {5--7},
  journal = {Computing in Science Engineering},
  keywords = {done},
  number = {1}
}

@article{foster1983,
  title = {An Axiomatic Characterization of the {{Theil}} Measure of Income Inequality},
  author = {Foster, James E},
  year = {1983},
  month = oct,
  volume = {31},
  pages = {105--121},
  issn = {0022-0531},
  doi = {10.1016/0022-0531(83)90023-6},
  abstract = {This paper provides a characterization of a frequently used measure of income inequality. It has been known for some time that the Theil measure of income inequality (1) is consistent with the Lorenz criterion, when it applies, and (2) exhibits a simple and empricically useful decomposition by population subgroup into within-group and between-group terms. The major theorem establishes the converse: the decomposability property defines the Theil measure uniquely (up to a positive multiple) among all Lorenz-consistent measures.},
  file = {/Users/ag/Zotero/storage/I5EBIRAL/0022053183900236.html},
  journal = {Journal of Economic Theory},
  number = {1}
}

@article{fotheringham1983,
  title = {A New Set of Spatial Interaction Models: The Theory of Competing Destinations},
  author = {Fotheringham, A S},
  year = {1983},
  volume = {15},
  pages = {1121--1132},
  date-added = {2015-10-20 11:35:51 +0000},
  date-modified = {2015-10-20 11:36:52 +0000},
  journal = {Environment and Planning A},
  keywords = {ocp022019_REGION_template}
}

@article{frankel2011,
  title = {Measuring School Segregation},
  author = {Frankel, David M. and Volij, Oscar},
  year = {2011},
  month = jan,
  volume = {146},
  pages = {1--38},
  issn = {0022-0531},
  doi = {10.1016/j.jet.2010.10.008},
  abstract = {Using only ordinal axioms, we characterize several multigroup school segregation indices: the Atkinson indices for the class of school districts with a given fixed number of ethnic groups and the Mutual Information index for the class of all districts. Properties of other school segregation indices are also discussed. In an empirical application, we document a weakening of the effect of ethnicity on school assignment from 1987/1988 to 2007/2008. We also show that segregation between districts within cities currently accounts for 33\% of total segregation. Segregation between states, driven mainly by the distinct residential patterns of Hispanics, contributes another 32\%.},
  file = {/Users/ag/Zotero/storage/V5NENWNG/Frankel and Volij - 2011 - Measuring school segregation.pdf;/Users/ag/Zotero/storage/RT2SYEKQ/S0022053110001353.html},
  journal = {Journal of Economic Theory},
  keywords = {Axiomatic method,Education,Indices,Measurement,Schools,Segregation},
  number = {1}
}

@article{frisch1933,
  title = {Editor's {{Note}}},
  author = {Frisch, Ragnar},
  year = {1933},
  volume = {1},
  pages = {1--4},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {1}
}

@article{frisch1933a,
  title = {Editor's {{Note}}},
  author = {Frisch, Ragnar},
  year = {1933},
  volume = {1},
  pages = {1--4},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {1}
}

@article{frischEditorNote1933,
  title = {Editor's {{Note}}},
  author = {Frisch, Ragnar},
  year = {1933},
  volume = {1},
  pages = {1--4},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {1}
}

@article{frischEditorNote1933a,
  title = {Editor's {{Note}}},
  author = {Frisch, Ragnar},
  year = {1933},
  volume = {1},
  pages = {1--4},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {00129682, 14680262},
  journal = {Econometrica},
  keywords = {done},
  number = {1}
}

@book{gastineau2002,
  title = {The {{Exchange}}-{{Traded Funds Manual}}},
  author = {Gastineau, Gary L.},
  year = {2002},
  month = feb,
  publisher = {{John Wiley \& Sons}},
  abstract = {Praise for the exchange-traded funds manual  "Exchange-traded funds are the hottest finance innovation of the past decade. Gary Gastineau, who played a critical role in their development, demystifies the working of these instruments, lucidly describes their advantages and disadvantages, and guides investors on their use. This gem of a book will be the ETF bible for years to come."  -Burton Malkiel, Chemical Bank Chairman's Professor of Economics, Princeton University  "This is the first comprehensive book on exchange-traded funds.The author displays an institutional and practical knowledge of exchange-traded funds that makes this book necessary reading for not only the knowledgeable investor but for the professional researcher seeking to understand these relatively new investment vehicles."  -Martin J. Gruber, Nomura Professor of Finance Stern School of Business, New York University  "Gary Gastineau is a national treasure. Exchange-traded funds are the wave of the future, and Gary has been instrumental in their development from day one. His knowledge is encyclopedic, and his style and subtle humor make it all accessible to the reader."  -Wayne H. Wagner, Chairman, Plexus Group, Inc.  "In Gary Gastineau's brilliant work in illuminating the reader on exchange-traded funds, he provides rich insights into the process and methodology of adding value and cites a convergence of market forces that creates a compelling story for the use of ETFs for those who choose to add value."  -Stephen C. Winks, Publisher, Senior Consultant  "The introduction of exchange-traded funds was one of the success stories of  Wall Street in the 1990s. Gary Gastineau was a key contributor to this success, and his book is an important benchmark on both the current status of this important new category and the vast potential of its next-generation products."  -Salvatore Sodano, Chairman and Chief Executive Officer American Stock Exchange},
  googlebooks = {CIYoyIrP6cIC},
  isbn = {978-0-471-21894-4},
  keywords = {Business \& Economics / General,Business \& Economics / Investments \& Securities / General},
  language = {en}
}

@article{gentleman2003,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  author = {Gentleman, Robert and Temple Lang, Duncan},
  year = {2003},
  volume = {16},
  pages = {1--23},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/106186007X178663},
  abstract = {For various reasons, it is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, etc. with the documents that describe and rely on them. This integration allows readers to both verify and adapt the statements in the documents. Authors can easily reproduce them in the future, and they can present the document's contents in a different medium, e.g. with interactive controls. This paper describes a software framework for authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents, including figures, tables, etc., can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences.},
  file = {/Users/ag/Zotero/storage/Y5EX8QC9/Gentleman og Temple Lang - 2007 - Statistical Analyses and Reproducible Research.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {1}
}

@article{gentleman2004,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  author = {Gentleman, Robert and Lang, Duncan Temple},
  year = {2004},
  month = may,
  file = {/Users/ag/Zotero/storage/H6NNTZSG/paper2.html},
  journal = {Bioconductor Project Working Papers}
}

@article{gentleman2005,
  title = {Reproducible {{Research}}: {{A Bioinformatics Case Study}}},
  shorttitle = {Reproducible {{Research}}},
  author = {Gentleman, Robert},
  year = {2005},
  month = jan,
  volume = {4},
  issn = {1544-6115, 2194-6302},
  doi = {10.2202/1544-6115.1034},
  abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output \textendash{} thereby providing the familiar, but limited, static document.},
  file = {/Users/ag/Zotero/storage/MYIWFKH4/Gentleman - 2005 - Reproducible Research A Bioinformatics Case Study.pdf},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  language = {en},
  number = {1}
}

@article{gentleman2005a,
  title = {Reproducible {{Research}}: {{A Bioinformatics Case Study}}},
  shorttitle = {Reproducible {{Research}}},
  author = {Gentleman, Robert},
  year = {2005},
  month = jan,
  volume = {4},
  issn = {1544-6115, 2194-6302},
  doi = {10.2202/1544-6115.1034},
  abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output \textendash{} thereby providing the familiar, but limited, static document.},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  keywords = {done},
  language = {English},
  number = {1}
}

@article{gentleman2005b,
  title = {Reproducible {{Research}}: {{A Bioinformatics Case Study}}},
  shorttitle = {Reproducible {{Research}}},
  author = {Gentleman, Robert},
  year = {2005},
  month = jan,
  volume = {4},
  issn = {1544-6115, 2194-6302},
  doi = {10.2202/1544-6115.1034},
  abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output \textendash{} thereby providing the familiar, but limited, static document.},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  keywords = {done},
  language = {English},
  number = {1}
}

@article{gentleman2007,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  author = {Gentleman, Robert and Temple Lang, Duncan},
  year = {2007},
  month = mar,
  volume = {16},
  pages = {1--23},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/106186007X178663},
  abstract = {For various reasons, it is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, etc. with the documents that describe and rely on them. This integration allows readers to both verify and adapt the statements in the documents. Authors can easily reproduce them in the future, and they can present the document's contents in a different medium, e.g. with interactive controls. This paper describes a software framework for authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents, including figures, tables, etc., can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences.},
  file = {/Users/ag/Zotero/storage/4P85W3K8/Gentleman og Temple Lang - 2007 - Statistical Analyses and Reproducible Research.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  language = {en},
  number = {1}
}

@article{gentleman2007a,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  author = {Gentleman, Robert and Lang, Duncan Temple},
  year = {2007},
  volume = {16},
  pages = {1--23},
  publisher = {{Taylor \& Francis}},
  doi = {10.1198/106186007X178663},
  abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents\textemdash{} including figures, tables, and so on\textemdash{} can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {done},
  number = {1}
}

@article{gentleman2007b,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  author = {Gentleman, Robert and Lang, Duncan Temple},
  year = {2007},
  volume = {16},
  pages = {1--23},
  publisher = {{Taylor \& Francis}},
  doi = {10.1198/106186007X178663},
  abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents\textemdash{} including figures, tables, and so on\textemdash{} can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {done},
  number = {1}
}

@article{gentlemanReproducibleResearchBioinformatics2005,
  title = {Reproducible {{Research}}: {{A Bioinformatics Case Study}}},
  shorttitle = {Reproducible {{Research}}},
  author = {Gentleman, Robert},
  year = {2005},
  month = jan,
  volume = {4},
  issn = {1544-6115, 2194-6302},
  doi = {10.2202/1544-6115.1034},
  abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output \textendash{} thereby providing the familiar, but limited, static document.},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  keywords = {done},
  language = {English},
  number = {1}
}

@article{gentlemanReproducibleResearchBioinformatics2005a,
  title = {Reproducible {{Research}}: {{A Bioinformatics Case Study}}},
  shorttitle = {Reproducible {{Research}}},
  author = {Gentleman, Robert},
  year = {2005},
  month = jan,
  volume = {4},
  issn = {1544-6115, 2194-6302},
  doi = {10.2202/1544-6115.1034},
  abstract = {While scientific research and the methodologies involved have gone through substantial technological evolution the technology involved in the publication of the results of these endeavors has remained relatively stagnant. Publication is largely done in the same manner today as it was fifty years ago. Many journals have adopted electronic formats, however, their orientation and style is little different from a printed document. The documents tend to be static and take little advantage of computational resources that might be available. Recent work, Gentleman and Temple Lang (2004), suggests a methodology and basic infrastructure that can be used to publish documents in a substantially different way. Their approach is suitable for the publication of papers whose message relies on computation. Stated quite simply, Gentleman and Temple Lang propose a paradigm where documents are mixtures of code and text. Such documents may be self-contained or they may be a component of a compendium which provides the infrastructure needed to provide access to data and supporting software. These documents, or compendiums, can be processed in a number of different ways. One transformation will be to replace the code with its output \textendash{} thereby providing the familiar, but limited, static document.},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  keywords = {done},
  language = {English},
  number = {1}
}

@article{gentlemanStatisticalAnalysesReproducible2007,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  author = {Gentleman, Robert and Lang, Duncan Temple},
  year = {2007},
  volume = {16},
  pages = {1--23},
  publisher = {{Taylor \& Francis}},
  doi = {10.1198/106186007X178663},
  abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents\textemdash{} including figures, tables, and so on\textemdash{} can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {done},
  number = {1}
}

@article{gentlemanStatisticalAnalysesReproducible2007a,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  author = {Gentleman, Robert and Lang, Duncan Temple},
  year = {2007},
  volume = {16},
  pages = {1--23},
  publisher = {{Taylor \& Francis}},
  doi = {10.1198/106186007X178663},
  abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents\textemdash{} including figures, tables, and so on\textemdash{} can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {done},
  number = {1}
}

@article{gitlesen2000,
  title = {A Competing Destinations Approach to Modeling Commuting Flows: {{A}} Theoretical Interpretation and an Empirical Application of the Model},
  author = {Gitlesen, J P and Thorsen, I},
  year = {2000},
  volume = {32},
  pages = {2057--2074},
  date-added = {2015-10-20 11:33:42 +0000},
  date-modified = {2015-10-20 11:35:44 +0000},
  journal = {Environment and Planning A},
  keywords = {ocp022019_REGION_template}
}

@article{gjerde2014,
  title = {{B\o rshandlede produkter: ETP, ETF og ETN}},
  shorttitle = {{B\o rshandlede produkter}},
  author = {Gjerde, {\O}ystein and S{\ae}ttem, Frode},
  year = {2014},
  volume = {30},
  pages = {367--380},
  publisher = {{Universitetsforlaget}},
  issn = {1504-2871, 1501-0074},
  abstract = {B\o rshandlede produkter (ETP - Exchange Traded Products) m\aa{} kunne omtales som en av de mest popul\ae re finansielle innovasjoner i l\o pet av de to siste ti\aa rene. I denne artikkelen redegj\o r vi for de to hovedgruppene av b\o rshandlede produkter, b\o rshandlede fond (ETF - Exchange Traded Funds) og b\o rshandlede verdipapirer (ETN - Exchange Traded Notes). I det norske markedet finnes det en rekke ETF- og ETN-produkter knyttet til OBX-indeksen. Vi dokumenterer at disse produktene har en lavere avkastningsmultippel m\aa lt mot indeksen enn hva produktene lover. Vi viser dessuten at likviditeten i de fleste ETP-produktene p\aa{} Oslo B\o rs er sv\ae rt tynn. Som bakteppe redegj\o r vi f\o rst kortfattet for sentrale forklaringsfaktorer for finansiell innovasjon og ulike kriterier som m\aa{} v\ae re oppfylte for at nyskapninger i finansmarkedet skal bli en suksess.},
  file = {/Users/ag/Zotero/storage/GBSAB6TF/Gjerde og Sættem - 2014 - Børshandlede produkter ETP, ETF og ETN.pdf;/Users/ag/Zotero/storage/ZILJRIEB/boershandlede_produkter_etp_etf_og_etn.html},
  journal = {Praktisk \o konomi \& finans},
  language = {no-NO},
  number = {04}
}

@article{gjestland2014,
  title = {The Suitability of Hedonic Models for Cost-Benefit Analysis: {{Evidence}} from Commuting Flows},
  author = {Gjestland, A and McArthur, D and Osland, L and Thorsen, I},
  year = {2014},
  volume = {61},
  pages = {136--151},
  date-added = {2015-10-20 11:35:51 +0000},
  date-modified = {2015-10-20 11:36:52 +0000},
  journal = {Transportation Research Part A: Policy and Practice},
  keywords = {ocp022019_REGION_template}
}

@article{golub1999,
  title = {Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring},
  shorttitle = {Molecular Classification of Cancer},
  author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
  year = {1999},
  month = oct,
  volume = {286},
  pages = {531--537},
  issn = {0036-8075},
  doi = {10.1126/science.286.5439.531},
  abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
  file = {/Users/ag/Zotero/storage/JYT7X2JI/Golub et al. - 1999 - Molecular classification of cancer class discover.pdf},
  journal = {Science (New York, N.Y.)},
  keywords = {Acute Disease,Antineoplastic Combined Chemotherapy Protocols,Cell Adhesion,Cell Cycle,Gene Expression Profiling,Homeodomain Proteins,Humans,Leukemia; Myeloid,Neoplasm Proteins,Neoplasms,Oligonucleotide Array Sequence Analysis,Oncogenes,Precursor Cell Lymphoblastic Leukemia-Lymphoma,Predictive Value of Tests,Reproducibility of Results,Treatment Outcome},
  language = {eng},
  number = {5439},
  pmid = {10521349}
}

@article{golub1999a,
  title = {Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Monitoring},
  shorttitle = {Molecular Classification of Cancer},
  author = {Golub, T.R. and Slonim, D.K. and Tamayo, Pablo and Huard, C and Gaasenbeek, M and Mesirov, J.P. and Coller, Hilary and Loh, Mignon and Downing, J.R. and Caligiuri, Michael and Bloomfield, C and Lander, E},
  year = {1999},
  month = nov,
  volume = {286},
  pages = {531--7},
  abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
  file = {/Users/ag/Zotero/storage/BMLJH6NJ/Golub et al. - 1999 - Molecular classification of cancer class discover.pdf},
  journal = {Science (New York, N.Y.)}
}

@article{golub1999b,
  title = {Molecular {{Classification}} of {{Cancer}}: {{Class Discovery}} and {{Class Prediction}} by {{Gene Expression Monitoring}}},
  shorttitle = {Molecular {{Classification}} of {{Cancer}}},
  author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
  year = {1999},
  month = oct,
  volume = {286},
  pages = {531--537},
  issn = {0036-8075},
  doi = {10.1126/science.286.5439.531},
  abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
  journal = {Science (New York, N.Y.)},
  keywords = {done},
  language = {English},
  number = {5439},
  pmid = {10521349}
}

@article{golub1999c,
  title = {Molecular {{Classification}} of {{Cancer}}: {{Class Discovery}} and {{Class Prediction}} by {{Gene Expression Monitoring}}},
  shorttitle = {Molecular {{Classification}} of {{Cancer}}},
  author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
  year = {1999},
  month = oct,
  volume = {286},
  pages = {531--537},
  issn = {0036-8075},
  doi = {10.1126/science.286.5439.531},
  abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
  journal = {Science (New York, N.Y.)},
  keywords = {done},
  language = {English},
  number = {5439},
  pmid = {10521349}
}

@article{golubMolecularClassificationCancer1999,
  title = {Molecular {{Classification}} of {{Cancer}}: {{Class Discovery}} and {{Class Prediction}} by {{Gene Expression Monitoring}}},
  shorttitle = {Molecular {{Classification}} of {{Cancer}}},
  author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
  year = {1999},
  month = oct,
  volume = {286},
  pages = {531--537},
  issn = {0036-8075},
  doi = {10.1126/science.286.5439.531},
  abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
  journal = {Science (New York, N.Y.)},
  keywords = {done},
  language = {English},
  number = {5439},
  pmid = {10521349}
}

@article{golubMolecularClassificationCancer1999a,
  title = {Molecular {{Classification}} of {{Cancer}}: {{Class Discovery}} and {{Class Prediction}} by {{Gene Expression Monitoring}}},
  shorttitle = {Molecular {{Classification}} of {{Cancer}}},
  author = {Golub, T. R. and Slonim, D. K. and Tamayo, P. and Huard, C. and Gaasenbeek, M. and Mesirov, J. P. and Coller, H. and Loh, M. L. and Downing, J. R. and Caligiuri, M. A. and Bloomfield, C. D. and Lander, E. S.},
  year = {1999},
  month = oct,
  volume = {286},
  pages = {531--537},
  issn = {0036-8075},
  doi = {10.1126/science.286.5439.531},
  abstract = {Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge.},
  journal = {Science (New York, N.Y.)},
  keywords = {done},
  language = {English},
  number = {5439},
  pmid = {10521349}
}

@article{goodman2016,
  title = {What Does Research Reproducibility Mean?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  volume = {8},
  pages = {341ps12-341ps12},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  file = {/Users/ag/Zotero/storage/HDKKJIR3/Goodman et al. - 2016 - What does research reproducibility mean.pdf},
  journal = {Science Translational Medicine},
  language = {en},
  number = {341}
}

@article{goodman2016a,
  title = {What {{Does Research Reproducibility Mean}}?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  volume = {8},
  pages = {341ps12-341ps12},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  journal = {Science Translational Medicine},
  keywords = {done},
  language = {English},
  number = {341}
}

@article{goodman2016b,
  title = {What {{Does Research Reproducibility Mean}}?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  volume = {8},
  pages = {341ps12-341ps12},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  journal = {Science Translational Medicine},
  keywords = {done},
  language = {English},
  number = {341}
}

@article{goodmanWhatDoesResearch2016,
  title = {What {{Does Research Reproducibility Mean}}?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  volume = {8},
  pages = {341ps12-341ps12},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  journal = {Science Translational Medicine},
  keywords = {done},
  language = {English},
  number = {341}
}

@article{goodmanWhatDoesResearch2016a,
  title = {What {{Does Research Reproducibility Mean}}?},
  author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
  year = {2016},
  month = jun,
  volume = {8},
  pages = {341ps12-341ps12},
  issn = {1946-6234, 1946-6242},
  doi = {10.1126/scitranslmed.aaf5027},
  journal = {Science Translational Medicine},
  keywords = {done},
  language = {English},
  number = {341}
}

@book{greene2003,
  title = {Econometric Analysis},
  author = {Greene, WH},
  year = {2003},
  publisher = {{Prentice Hall}},
  address = {{New Jersey}},
  date-modified = {2012-02-28 12:24:08 +0000}
}

@book{grolemund,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots\textemdash and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
  file = {/Users/ag/Zotero/storage/ZVWKI7ZV/r4ds.had.co.nz.html}
}

@book{grolemunda,
  title = {R {{Markdown}}: {{The Definitive Guide}}},
  shorttitle = {R {{Markdown}}},
  author = {Grolemund, J. J. Allaire, Garrett, Yihui Xie},
  abstract = {The first official book authored by the core R Markdown developers that provides a comprehensive and accurate reference to the R Markdown ecosystem. With R Markdown, you can easily create reproducible data analysis reports, presentations, dashboards, interactive applications, books, dissertations, websites, and journal articles, while enjoying the simplicity of Markdown and the great power of R and other languages.},
  file = {/Users/ag/Zotero/storage/XKXMVKYP/rmarkdown.html}
}

@book{grolemundb,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots\textemdash{} and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.}
}

@book{grolemundc,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots\textemdash{} and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.}
}

@book{grolemundDataScience,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots\textemdash{} and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.}
}

@book{grolemundDataSciencea,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots\textemdash{} and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.}
}

@manual{grosjean2018,
  title = {Pastecs: {{Package}} for Analysis of Space-Time Ecological Series},
  author = {Grosjean, Philippe and Ibanez, Frederic},
  year = {2018},
  type = {Manual}
}

@book{gruber,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@book{grubera,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@book{gruberb,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@book{gruberc,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@book{gruberDaringFireballMarkdown,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@book{gruberDaringFireballMarkdowna,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@book{gruberDaringFireballMarkdownb,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@book{gruberDaringFireballMarkdownc,
  title = {Daring {{Fireball}}: {{Markdown}}},
  author = {Gruber, John and Swartz, Aron},
  keywords = {done}
}

@article{grytten2016,
  title = {Norske B\o rshandlede Produkter: En Analyse Av {{ETN}} Og {{ETF}}},
  shorttitle = {Norske B\o rshandlede Produkter},
  author = {Grytten, Linn},
  year = {2016},
  abstract = {Masteroppgave i bedrifts\o konomi - Nord universitet, 201},
  file = {/Users/ag/Zotero/storage/E9JCJZIK/Grytten - 2016 - Norske børshandlede produkter en analyse av ETN o.pdf;/Users/ag/Zotero/storage/DNU6N5KX/52138349.html},
  language = {en-gb}
}

@article{hamilton1982,
  title = {Wasteful Commuting},
  author = {Hamilton, BW},
  year = {1982},
  volume = {90},
  pages = {1497--1504},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Political Economy}
}

@article{handy1997,
  title = {Measuring Accessibility: An Exploration of Issues and Alternatives},
  author = {Handy, SL and Niemeier, DA},
  year = {1997},
  volume = {29},
  pages = {1175--1194},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Environment and Planning A},
  keywords = {ocp022019_REGION_template}
}

@article{hansen1959,
  title = {How Accessibility Shapes Land Use},
  author = {Hansen, WG},
  year = {1959},
  volume = {25},
  pages = {73--76},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of the American Institute of Planners},
  keywords = {ocp022019_REGION_template}
}

@manual{harrelljr2020,
  title = {Hmisc: {{Harrell}} Miscellaneous},
  author = {Harrell Jr, Frank E and Dupont, with contributions from Charles and {others.}, many},
  year = {2020},
  type = {Manual}
}

@book{hastie1990,
  title = {Generalized Additive Models},
  author = {Hastie, T and Tibshirani, RJ},
  year = {1990},
  publisher = {{Chapman and Hall}},
  address = {{London}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@article{heikkila1989,
  title = {What Happened to the {{CBD}}-Distance Gradient?: Land Values in a Polycentric City},
  author = {Heikkila, E and Gordon, P and Kim, JI and Peiser, RB and Richardson, HW},
  year = {1989},
  volume = {21},
  pages = {221--232},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Environment and Planning A},
  keywords = {ocp022019_REGION_template}
}

@article{holt1996,
  title = {Aggregation and Ecological Effects in Geographically Based Data},
  author = {Holt, D and Steel, D and Tranmer, M and Wrigley, N},
  year = {1996},
  volume = {28},
  pages = {244--261},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Geographical Analysis}
}

@article{hoogstra2017,
  title = {Do Jobs Follow People or People Follow Jobs? {{A}} Meta-Analysis of {{Carlino}}\textendash{{Mills}} Studies},
  author = {Hoogstra, Gerke J. and {van Dijk}, Jouke and Florax, Raymond J.G.M.},
  year = {2017},
  month = oct,
  volume = {12},
  pages = {357--378},
  publisher = {{Routledge}},
  issn = {1742-1772},
  doi = {10.1080/17421772.2017.1340663},
  abstract = {Do jobs follow people or people follow jobs? A meta-analysis of Carlino\textendash Mills studies. Spatial Economic Analysis. This study examines the classic question as to whether `jobs follow people' or `people follow jobs' by performing a meta-analysis of 321 results from 64 Carlino\textendash Mills studies. It is found that the results are highly divergent, but that more results point towards `jobs following people' than towards `people following jobs'. When it comes to the reasons for the variation in results, we find that the results are mostly shaped by the geographical location, spatial resolution, and population and employment characteristics present in the data, as well as by the model's specification, its functional form and the spatial weight matrix specification.},
  journal = {Spatial Economic Analysis},
  keywords = {adjustment model,Carlino–Mills model,jobs–people causality,meta-analysis,Population–employment interaction,simultaneous equations},
  language = {English},
  number = {4}
}

@article{hughes1992,
  title = {Traffic Externalities and Single-Family House Prices},
  author = {Hughes, JWT and Sirmans, CF},
  year = {1992},
  volume = {32},
  pages = {487--500},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Regional Science},
  number = {4}
}

@article{hull2012,
  title = {Options, Futures, and Other Derivatives},
  author = {Hull, John},
  year = {2012},
  edition = {8th ed., global ed.},
  publisher = {Pearson},
  location = {Boston, Mass},
  isbn = {9780273759072},
  keywords = {aksjer,børsspekulasjon,Capital market,derivater,Derivater,Economic analysis,Financing,futures,investering,investeringer,Investeringer,investeringsteori,Investment,Kapitalmarked,Økonomi,opsjoner,Opsjoner,opsjonshandel,Opsjonshandel : Spekulasjon,opsjonsmarkeder,Opsjonsmarkeder,porteføjeteori,Terminmarked,verdipapirer,Verdipapirer,Verdipapirer : Investering : Bank- og pengevesen,verdipapirhandel},
  language = {English}
}

@book{hull2014,
  title = {Students Solutions Manual \& Study Guide for {{Fundamentals}} of Futures and Options Markets, Eighth Edition},
  author = {Hull, John and Hull, John},
  year = {2014},
  annotation = {OCLC: 865160008},
  isbn = {978-1-292-04162-9},
  language = {English}
}

@book{hull2015,
  title = {Risk Management and Financial Institutions},
  author = {Hull, John},
  year = {2015},
  edition = {Fourth edition},
  publisher = {{Wiley}},
  address = {{Hoboken, New Jersey}},
  abstract = {Business snapshots -- Preface -- Introduction -- Financial institutions and their trading -- Banks -- Insurance companies and pension plans -- Mutual funds and hedge funds -- Appendix a: compounding frequencies and interest rates -- Appendix b: zero rates, forward rates, and zero-coupon yield curves -- Appendix c: valuing forward and futures contracts -- Appendix d: valuing swaps -- Appendix e: valuing european options -- Appendix f: valuing american options -- Appendix g: taylor series expansions -- Appendix h: eigenvectors and eigenvalues -- Appendix i: principal components analysis -- Appendix j: manipulation of credit transition matrices -- Appendix k: valuation of credit default swaps -- Appendix l: synthetic cdos and their valuation -- Answers to questions and problems -- Glossary of terms -- Derivagem software},
  annotation = {OCLC: 908622504},
  file = {/Users/ag/Zotero/storage/2CM4NSRB/Hull - 2015 - Risk management and financial institutions.pdf},
  isbn = {978-1-118-95595-6 978-1-118-95594-9 978-1-118-95596-3},
  language = {eng},
  series = {Wiley Finance Series}
}

@book{hull2018,
  title = {Options, Futures, and Other Derivatives},
  author = {Hull, John},
  year = {2018},
  edition = {9th ed., global ed.},
  publisher = {{Pearson}},
  address = {{Harlow}},
  isbn = {978-1-292-21289-0},
  keywords = {aksjer,børsspekulasjon,derivater,futures,investering,investeringer,investeringsteori,økonomi,opsjoner,opsjonshandel,opsjonsmarkeder,porteføljeteori,verdipapirer,verdipapirhandel},
  language = {eng},
  lccn = {336.764.2 HUL, 332.63 Hul, H, 336.764.2 H87o/9.utg., 332.63}
}

@inproceedings{ihaka1996,
  title = {Gentleman {{R}}: {{R}}: {{A}} Language for Data Analysis and Graphics},
  author = {Ihaka, Ross},
  year = {1996}
}

@article{ioannidis2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  file = {/Users/ag/Zotero/storage/9HIDLC24/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf;/Users/ag/Zotero/storage/I2MHG9U3/article.html},
  journal = {PLOS Medicine},
  keywords = {Cancer risk factors,Finance,Genetic epidemiology,Genetics of disease,Metaanalysis,Randomized controlled trials,Research design,Schizophrenia},
  language = {en},
  number = {8}
}

@article{ioannidis2005a,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  journal = {PLOS Medicine},
  keywords = {done},
  language = {English},
  number = {8}
}

@article{ioannidis2005b,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  journal = {PLOS Medicine},
  keywords = {done},
  language = {English},
  number = {8}
}

@article{ioannidisWhyMostPublished2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  journal = {PLOS Medicine},
  keywords = {done},
  language = {English},
  number = {8}
}

@article{ioannidisWhyMostPublished2005a,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  volume = {2},
  pages = {e124},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  journal = {PLOS Medicine},
  keywords = {done},
  language = {English},
  number = {8}
}

@article{iyengar1988,
  title = {Selection Models and the File Drawer Problem},
  author = {Iyengar, Satish and Greenhouse, Joel B.},
  year = {1988},
  volume = {3},
  pages = {109--117},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
  journal = {Statistical Science},
  number = {1}
}

@article{iyengar1988a,
  title = {Selection {{Models}} and the {{File Drawer Problem}}},
  author = {Iyengar, Satish and Greenhouse, Joel B.},
  year = {1988},
  volume = {3},
  pages = {109--117},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
  journal = {Statistical Science},
  keywords = {done},
  number = {1}
}

@article{iyengar1988b,
  title = {Selection {{Models}} and the {{File Drawer Problem}}},
  author = {Iyengar, Satish and Greenhouse, Joel B.},
  year = {1988},
  volume = {3},
  pages = {109--117},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
  journal = {Statistical Science},
  keywords = {done},
  number = {1}
}

@article{iyengarSelectionModelsFile1988,
  title = {Selection {{Models}} and the {{File Drawer Problem}}},
  author = {Iyengar, Satish and Greenhouse, Joel B.},
  year = {1988},
  volume = {3},
  pages = {109--117},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
  journal = {Statistical Science},
  keywords = {done},
  number = {1}
}

@article{iyengarSelectionModelsFile1988a,
  title = {Selection {{Models}} and the {{File Drawer Problem}}},
  author = {Iyengar, Satish and Greenhouse, Joel B.},
  year = {1988},
  volume = {3},
  pages = {109--117},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
  journal = {Statistical Science},
  keywords = {done},
  number = {1}
}

@article{jackson1979,
  title = {Intraurban Variation in the Price of Housing},
  author = {Jackson, JR},
  year = {1979},
  volume = {6},
  pages = {464--479},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Urban Economics},
  keywords = {ocp022019_REGION_template}
}

@article{jasny2011,
  title = {Again, and Again, and Again},
  author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
  year = {2011},
  volume = {334},
  pages = {1225--1225},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.334.6060.1225},
  eprint = {https://science.sciencemag.org/content/334/6060/1225.full.pdf},
  journal = {Science},
  number = {6060}
}

@article{jasny2011a,
  title = {Again, and {{Again}}, and {{Again}}},
  author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
  year = {2011},
  volume = {334},
  pages = {1225--1225},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.334.6060.1225},
  journal = {Science},
  keywords = {done},
  number = {6060}
}

@article{jasny2011b,
  title = {Again, and {{Again}}, and {{Again}}},
  author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
  year = {2011},
  volume = {334},
  pages = {1225--1225},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.334.6060.1225},
  journal = {Science},
  keywords = {done},
  number = {6060}
}

@article{jasnyAgainAgainAgain2011,
  title = {Again, and {{Again}}, and {{Again}}},
  author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
  year = {2011},
  volume = {334},
  pages = {1225--1225},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.334.6060.1225},
  journal = {Science},
  keywords = {done},
  number = {6060}
}

@article{jasnyAgainAgainAgain2011a,
  title = {Again, and {{Again}}, and {{Again}}},
  author = {Jasny, Barbara R. and Chin, Gilbert and Chong, Lisa and Vignieri, Sacha},
  year = {2011},
  volume = {334},
  pages = {1225--1225},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.334.6060.1225},
  journal = {Science},
  keywords = {done},
  number = {6060}
}

@article{judge2004,
  title = {The {{Effect}} of {{Physical Height}} on {{Workplace Success}} and {{Income}}: {{Preliminary Test}} of a {{Theoretical Model}}.},
  shorttitle = {The {{Effect}} of {{Physical Height}} on {{Workplace Success}} and {{Income}}},
  author = {Judge, Timothy A. and Cable, Daniel M.},
  year = {2004},
  volume = {89},
  pages = {428--441},
  issn = {1939-1854, 0021-9010},
  doi = {10.1037/0021-9010.89.3.428},
  file = {/Users/ag/Zotero/storage/S39RZTSJ/Judge og Cable - 2004 - The Effect of Physical Height on Workplace Success.pdf},
  journal = {Journal of Applied Psychology},
  language = {en},
  number = {3}
}

@article{kanbur2019,
  title = {Inequality {{Indices}} as {{Tests}} of {{Fairness}}},
  author = {Kanbur, Ravi and Snell, Andy},
  year = {2019},
  month = jul,
  volume = {129},
  pages = {2216--2239},
  issn = {0013-0133},
  doi = {10.1111/ecoj.12637},
  abstract = {Inequality indices are traditionally interpreted as measures of deviations from equality. This article interprets them instead as statistical tests for a null of fairness within well-defined income generating processes. We find that the likelihood ratio (LR) test for fairness versus unfairness within two such processes are proportional to Theil's first and second inequality indices respectively. The LR values may be used either as a test statistic or to approximate a Bayes factor that measures the posterior probabilities of the fair version of the processes over that of the unfair. We also apply this perspective to measurement of inequality of opportunity.},
  annotation = {WOS:000477674000011},
  file = {/Users/ag/Zotero/storage/EWVMNKMZ/Kanbur and Snell - 2019 - Inequality Indices as Tests of Fairness.pdf},
  journal = {Economic Journal},
  keywords = {equality,opportunity theory,risk,welfare},
  language = {English},
  number = {621}
}

@article{kirby2009,
  title = {Changes in Commuting to Work Times over the 1990 to 2000 Period},
  author = {Kirby, DK and LeSage, J},
  year = {2009},
  volume = {39},
  pages = {460--471},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Regional Science and Urban Economics},
  keywords = {ocp022019_REGION_template}
}

@article{klein2018,
  title = {Many Labs 2: {{Investigating}} Variation in Replicability across Samples and Settings},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and {de Bruijn}, Maaike and Schutter, Leander De and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and Echeverr{\'i}a, Alejandro V{\'a}squez- and Vaughn, Leigh Ann and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  volume = {1},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p \textexclamdown{} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p \textexclamdown{} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (\textexclamdown{} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  eprint = {https://doi.org/10.1177/2515245918810225},
  journal = {Advances in Methods and Practices in Psychological Science},
  number = {4}
}

@article{klein2018a,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability}} across {{Samples}} and {{Settings}}},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and {de Bruijn}, Maaike and Schutter, Leander De and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\textbackslash}AAse H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}djedovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and Echeverr{\'i}a, Alejandro V{\'a}squez- and Vaughn, Leigh Ann and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  volume = {1},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p \textexclamdown{} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p \textexclamdown{} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (\textexclamdown{} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  journal = {Advances in Methods and Practices in Psychological Science},
  keywords = {done},
  number = {4}
}

@article{klein2018b,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability}} across {{Samples}} and {{Settings}}},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and {de Bruijn}, Maaike and Schutter, Leander De and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\textbackslash}AAse H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}djedovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and Echeverr{\'i}a, Alejandro V{\'a}squez- and Vaughn, Leigh Ann and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  volume = {1},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p \textexclamdown{} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p \textexclamdown{} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (\textexclamdown{} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  journal = {Advances in Methods and Practices in Psychological Science},
  keywords = {done},
  number = {4}
}

@article{kleinManyLabsInvestigating2018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability}} across {{Samples}} and {{Settings}}},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and {de Bruijn}, Maaike and Schutter, Leander De and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\textbackslash}AAse H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}djedovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and Echeverr{\'i}a, Alejandro V{\'a}squez- and Vaughn, Leigh Ann and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  volume = {1},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p \textexclamdown{} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p \textexclamdown{} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (\textexclamdown{} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  journal = {Advances in Methods and Practices in Psychological Science},
  keywords = {done},
  number = {4}
}

@article{kleinManyLabsInvestigating2018a,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability}} across {{Samples}} and {{Settings}}},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and {de Bruijn}, Maaike and Schutter, Leander De and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\textbackslash}AAse H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\textbackslash}djedovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and Echeverr{\'i}a, Alejandro V{\'a}squez- and Vaughn, Leigh Ann and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  volume = {1},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p \textexclamdown{} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p \textexclamdown{} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (\textexclamdown{} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  journal = {Advances in Methods and Practices in Psychological Science},
  keywords = {done},
  number = {4}
}

@article{knuth1984,
  title = {Literate {{Programming}}},
  author = {Knuth, D. E.},
  year = {1984},
  month = jan,
  volume = {27},
  pages = {97--111},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
  journal = {The Computer Journal},
  keywords = {done},
  number = {2}
}

@article{knuth1984a,
  title = {Literate {{Programming}}},
  author = {Knuth, D. E.},
  year = {1984},
  month = jan,
  volume = {27},
  pages = {97--111},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
  journal = {The Computer Journal},
  keywords = {done},
  number = {2}
}

@book{knuth1986,
  title = {The {{TeXbook}}},
  author = {Knuth, D.E.},
  year = {1986},
  publisher = {{Addison-Wesley}},
  isbn = {978-0-201-13447-6},
  lccn = {85030845},
  series = {Computers \& Typesetting}
}

@book{knuth1986a,
  title = {The {{TeXbook}}},
  author = {Knuth, D.E.},
  year = {1986},
  publisher = {{Addison-Wesley}},
  isbn = {978-0-201-13447-6},
  keywords = {done},
  lccn = {85030845},
  series = {Computers \& {{Typesetting}}}
}

@book{knuth1986b,
  title = {The {{TeXbook}}},
  author = {Knuth, D.E.},
  year = {1986},
  publisher = {{Addison-Wesley}},
  isbn = {978-0-201-13447-6},
  keywords = {done},
  lccn = {85030845},
  series = {Computers \& {{Typesetting}}}
}

@book{knuth1992,
  title = {Literate {{Programming}}},
  author = {Knuth, Donald E.},
  year = {1992},
  month = mar,
  publisher = {{Cambridge University Press}},
  abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
  googlebooks = {vovpQgAACAAJ},
  isbn = {978-0-937073-81-0},
  keywords = {Computers / Social Aspects},
  language = {en}
}

@book{knuth1992a,
  title = {Literate {{Programming}}},
  author = {Knuth, Donald E.},
  year = {1992},
  month = mar,
  publisher = {{Cambridge University Press}},
  abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
  isbn = {978-0-937073-81-0},
  keywords = {done},
  language = {English}
}

@book{knuth1992b,
  title = {Literate {{Programming}}},
  author = {Knuth, Donald E.},
  year = {1992},
  month = mar,
  publisher = {{Cambridge University Press}},
  abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
  isbn = {978-0-937073-81-0},
  keywords = {done},
  language = {English}
}

@article{knuthLiterateProgramming1984,
  title = {Literate {{Programming}}},
  author = {Knuth, D. E.},
  year = {1984},
  month = jan,
  volume = {27},
  pages = {97--111},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
  journal = {The Computer Journal},
  keywords = {done},
  number = {2}
}

@article{knuthLiterateProgramming1984a,
  title = {Literate {{Programming}}},
  author = {Knuth, D. E.},
  year = {1984},
  month = jan,
  volume = {27},
  pages = {97--111},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
  journal = {The Computer Journal},
  keywords = {done},
  number = {2}
}

@book{knuthLiterateProgramming1992,
  title = {Literate {{Programming}}},
  author = {Knuth, Donald E.},
  year = {1992},
  month = mar,
  publisher = {{Cambridge University Press}},
  abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
  isbn = {978-0-937073-81-0},
  keywords = {done},
  language = {English}
}

@book{knuthLiterateProgramming1992a,
  title = {Literate {{Programming}}},
  author = {Knuth, Donald E.},
  year = {1992},
  month = mar,
  publisher = {{Cambridge University Press}},
  abstract = {Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself.},
  isbn = {978-0-937073-81-0},
  keywords = {done},
  language = {English}
}

@book{knuthTeXbook1986,
  title = {The {{TeXbook}}},
  author = {Knuth, D.E.},
  year = {1986},
  publisher = {{Addison-Wesley}},
  isbn = {978-0-201-13447-6},
  keywords = {done},
  lccn = {85030845},
  series = {Computers \& {{Typesetting}}}
}

@book{knuthTeXbook1986a,
  title = {The {{TeXbook}}},
  author = {Knuth, D.E.},
  year = {1986},
  publisher = {{Addison-Wesley}},
  isbn = {978-0-201-13447-6},
  keywords = {done},
  lccn = {85030845},
  series = {Computers \& {{Typesetting}}}
}

@misc{kommentator,
  title = {{Europas sikkerhet er i spill. USA kan havne p\aa{} et sidespor, og hva skjer i Tyskland? | Frank Rossavik}},
  shorttitle = {{Europas sikkerhet er i spill. USA kan havne p\aa{} et sidespor, og hva skjer i Tyskland?}},
  author = {Kommentator, Frank Rossavik},
  abstract = {NATO trygger ikke freden.},
  file = {/Users/ag/Zotero/storage/YUEQ5WLH/Europas-sikkerhet-er-i-spill-USA-kan-havne-pa-et-sidespor_-og-hva-skjer-i-Tyskland--Frank-Rossa.html},
  howpublished = {https://www.aftenposten.no/article/ap-6jqabW.html},
  journal = {Aftenposten},
  language = {nb-NO}
}

@misc{kunnskapsdepartementet2018,
  title = {{NOU 2018: 15}},
  shorttitle = {{NOU 2018}},
  author = {Kunnskapsdepartementet},
  year = {2018},
  month = dec,
  abstract = {Ragnhild Lied leder et utvalg som ble nedsatt av Kunnskapsdepartementet 1. september 2017. Liedutvalget har med dette overlevert den f\o rste av to utredninger.   10. desember 2018 overleverte utvalget utredningen Kvalifisert, forberedt og motivert. Et k...},
  file = {/Users/ag/Zotero/storage/PV4AF6S5/id2621801.html},
  howpublished = {https://www.regjeringen.no/no/dokumenter/nou-2018-15/id2621801/},
  journal = {Regjeringen.no},
  language = {no},
  type = {{NOU}}
}

@article{kwan2013,
  title = {Recent Advances in Accessibility Research: Representation, Methodology, and Applications},
  author = {Kwan, M-P and Murray, A T and O'Kelly, M E and Tiefelsdorf, M},
  year = {2013},
  volume = {5},
  pages = {129--138},
  date-added = {2020-01-09 14:57:14 +0100},
  date-modified = {2020-01-09 15:01:16 +0100},
  journal = {Journal of Geographical Systems},
  keywords = {ocp022019_REGION_template},
  number = {1}
}

@book{lamport1986,
  title = {{{LATEX}}: A Document Preparation System},
  shorttitle = {{{LATEX}}},
  author = {Lamport, Leslie},
  year = {1986},
  pages = {XIV, 242},
  publisher = {{Addison-Wesley}},
  address = {{Reading, Mass}},
  isbn = {978-0-201-15790-1},
  keywords = {Databehandling; LATEX; Tekstbehandling; databehandling; desktop; publishing; typografi; trykking; informatikk; tex; latex; programvare; tekstbehandling; latex(program); satseteknikk; programmering; grafisk; grensesnitt; databasert; Typografi,Databehandling; Tekstbehandling; latex; tex; markeringsspråk; Typografi : Databehandling,LaTeX (Computer system); Computerized typesetting; Vitenskapelig publisering; LaTeX; Digital typografi; TeX; Tekstbehandling; Typografi; Informatikk; Programmering; Databehandling; Programvare; Typografi},
  language = {eng},
  lccn = {681.3:655 Lam, 686.0285 Lam, 681.3 LA, 10308, 8348, 681.3:655 L19l, 686.22544 Lam, 686.22544 L, 91-034, 91-210, 93-081, I.7.2 LAM, I.7.2 LaTeX/Lam, 006.74 TEX Lam, 142/142, I.7.2 Lam, 1954}
}

@book{lander2017,
  title = {R for {{Everyone}}: {{Advanced Analytics}} and {{Graphics}}},
  shorttitle = {R for {{Everyone}}},
  author = {Lander, Jared P.},
  year = {2017},
  month = jun,
  edition = {2nd Edition},
  publisher = {{Addison-Wesley Professional}},
  address = {{Boston}},
  abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
  isbn = {978-0-13-454692-6},
  language = {English}
}

@book{lander2017a,
  title = {R for {{Everyone}}: {{Advanced Analytics}} and {{Graphics}}},
  shorttitle = {R for {{Everyone}}},
  author = {Lander, Jared P.},
  year = {2017},
  month = jun,
  edition = {2nd Edition},
  publisher = {{Addison-Wesley Professional}},
  address = {{Boston}},
  abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
  isbn = {978-0-13-454692-6},
  language = {English}
}

@book{landerEveryoneAdvancedAnalytics2017,
  title = {R for {{Everyone}}: {{Advanced Analytics}} and {{Graphics}}},
  shorttitle = {R for {{Everyone}}},
  author = {Lander, Jared P.},
  year = {2017},
  month = jun,
  edition = {2nd Edition},
  publisher = {{Addison-Wesley Professional}},
  address = {{Boston}},
  abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
  isbn = {978-0-13-454692-6},
  language = {English}
}

@book{landerEveryoneAdvancedAnalytics2017a,
  title = {R for {{Everyone}}: {{Advanced Analytics}} and {{Graphics}}},
  shorttitle = {R for {{Everyone}}},
  author = {Lander, Jared P.},
  year = {2017},
  month = jun,
  edition = {2nd Edition},
  publisher = {{Addison-Wesley Professional}},
  address = {{Boston}},
  abstract = {Statistical Computation for Programmers, Scientists, Quants, Excel Users, and Other Professionals Using the open source R language, you can build powerful statistical models to answer many of your most challenging questions. R has traditionally been difficult for non-statisticians to learn, and most R books assume far too much knowledge to be of help. R for Everyone, Second Edition, is the solution. Drawing on his unsurpassed experience teaching new users, professional data scientist Jared P. Lander has written the perfect tutorial for anyone new to statistical programming and modeling. Organized to make learning easy and intuitive, this guide focuses on the 20 percent of R functionality you'll need to accomplish 80 percent of modern data tasks. Lander's self-contained chapters start with the absolute basics, offering extensive hands-on practice and sample code. You'll download and install R; navigate and use the R environment; master basic program control, data import, manipulation, and visualization; and walk through several essential tests. Then, building on this foundation, you'll construct several complete models, both linear and nonlinear, and use some data mining techniques. After all this you'll make your code reproducible with LaTeX, RMarkdown, and Shiny. By the time you're done, you won't just know how to write R programs, you'll be ready to tackle the statistical problems you care about most. Coverage includes Explore R, RStudio, and R packages Use R for math: variable types, vectors, calling functions, and more Exploit data structures, including data.frames, matrices, and lists Read many different types of data Create attractive, intuitive statistical graphics Write user-defined functions Control program flow with if, ifelse, and complex checks Improve program efficiency with group manipulations Combine and reshape multiple datasets Manipulate strings using R's facilities and regular expressions Create normal, binomial, and Poisson probability distributions Build linear, generalized linear, and nonlinear models Program basic statistics: mean, standard deviation, and t-tests Train machine learning models Assess the quality of models and variable selection Prevent overfitting and perform variable selection, using the Elastic Net and Bayesian methods Analyze univariate and multivariate time series data Group data via K-means and hierarchical clustering Prepare reports, slideshows, and web pages with knitr Display interactive data with RMarkdown and htmlwidgets Implement dashboards with Shiny Build reusable R packages with devtools and Rcpp Register your product at informit.com/register for convenient access to downloads, updates, and corrections as they become available.},
  isbn = {978-0-13-454692-6},
  language = {English}
}

@incollection{leisch2002,
  title = {Sweave: {{Dynamic Generation}} of {{Statistical Reports Using Literate Data Analysis}}},
  shorttitle = {Sweave},
  booktitle = {Compstat},
  author = {Leisch, Friedrich},
  editor = {H{\"a}rdle, Wolfgang and R{\"o}nz, Bernd},
  year = {2002},
  pages = {575--580},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-642-57489-4_89},
  abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
  isbn = {978-3-7908-1517-7 978-3-642-57489-4},
  keywords = {done},
  language = {English}
}

@incollection{leisch2002a,
  title = {Sweave: {{Dynamic Generation}} of {{Statistical Reports Using Literate Data Analysis}}},
  shorttitle = {Sweave},
  booktitle = {Compstat},
  author = {Leisch, Friedrich},
  editor = {H{\"a}rdle, Wolfgang and R{\"o}nz, Bernd},
  year = {2002},
  pages = {575--580},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-642-57489-4_89},
  abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
  isbn = {978-3-7908-1517-7 978-3-642-57489-4},
  keywords = {done},
  language = {English}
}

@incollection{leischSweaveDynamicGeneration2002,
  title = {Sweave: {{Dynamic Generation}} of {{Statistical Reports Using Literate Data Analysis}}},
  shorttitle = {Sweave},
  booktitle = {Compstat},
  author = {Leisch, Friedrich},
  editor = {H{\"a}rdle, Wolfgang and R{\"o}nz, Bernd},
  year = {2002},
  pages = {575--580},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-642-57489-4_89},
  abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
  isbn = {978-3-7908-1517-7 978-3-642-57489-4},
  keywords = {done},
  language = {English}
}

@incollection{leischSweaveDynamicGeneration2002a,
  title = {Sweave: {{Dynamic Generation}} of {{Statistical Reports Using Literate Data Analysis}}},
  shorttitle = {Sweave},
  booktitle = {Compstat},
  author = {Leisch, Friedrich},
  editor = {H{\"a}rdle, Wolfgang and R{\"o}nz, Bernd},
  year = {2002},
  pages = {575--580},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-642-57489-4_89},
  abstract = {Sweave combines typesetting with LATEX and data anlysis with S into integrated statistical documents. When run through R or Splus, all data analysis output (tables, graphs, . . . ) is created on the fly and inserted into a final LATEX document. Options control which parts of the original S code are shown to or hidden from the reader, respectively. Many S users are also LATEX users, hence no new software has to be learned. The report can be automatically updated if data or analysis change, which allows for truly reproducible research.},
  isbn = {978-3-7908-1517-7 978-3-642-57489-4},
  keywords = {done},
  language = {English}
}

@article{lesage2008,
  title = {Spatial Growth Regressions: Model Specification, Estimation and Interpretation},
  author = {LeSage, J and Fischer, MM},
  year = {2008},
  volume = {3},
  publisher = {{Taylor and Francis Journals}},
  address = {{275-304}},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Spatial Economic Analysis},
  keywords = {ocp022019_REGION_template},
  number = {3}
}

@book{lesage2009,
  title = {Introduction to Spatial Econometrics},
  author = {LeSage, J and Pace, RK},
  year = {2009},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@article{li1980,
  title = {Micro-Neighbourhood Externalities and Hedonic Prices},
  author = {Li, MM and Brown, HJ},
  year = {1980},
  volume = {56},
  pages = {125--140},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Land Economics},
  keywords = {ocp022019_REGION_template}
}

@article{lovik2018,
  title = {{pollenallergi}},
  author = {L{\o}vik, Martinus},
  year = {2018},
  month = sep,
  abstract = {Pollenallergi, eller pollinose, er allergi mot pollen, blomsterst\o v fr\aa{} planter og tre. Allergi er \aa rsak til h\o ysnue, som er ei form for allergisk rinokonjunktivitt, og kan ogs\aa{} gje astma.Pollenallergi er mest vanleg i alderen fr\aa{} fem til 40 \aa r. Omlag ein person av fem i befolkninga har pollenallergi av ulik styrke. \AA tte av ti astmatikarar har pollenallergi, omlag ein av fem pollenallergikarar har astma.Pollenallergikarar kan ha rett til forlenga tid p\aa{} eksamen, med legeattest..},
  copyright = {fri},
  file = {/Users/ag/Zotero/storage/AVC6IHDA/pollenallergi.html},
  journal = {Store medisinske leksikon},
  keywords = {Allergologi},
  language = {no}
}

@article{lowndes2017,
  title = {Our Path to Better Science in Less Time Using Open Data Science Tools},
  author = {Lowndes, Julia S. Stewart and Best, Benjamin D. and Scarborough, Courtney and Afflerbach, Jamie C. and Frazier, Melanie R. and O'Hara, Casey C. and Jiang, Ning and Halpern, Benjamin S.},
  year = {2017},
  month = jun,
  volume = {1},
  pages = {0160},
  issn = {2397-334X},
  doi = {10.1038/s41559-017-0160},
  abstract = {Reproducibility has long been a tenet of science but has been challenging to achieve\textemdash we learned this the hard way when our old approaches proved inadequate to efficiently reproduce our own work. Here we describe how several free software tools have fundamentally upgraded our approach to collaborative research, making our entire workflow more transparent and streamlined. By describing specific tools and how we incrementally began using them for the Ocean Health Index project, we hope to encourage others in the scientific community to do the same\textemdash so we can all produce better science in less time.},
  copyright = {2017 Nature Publishing Group},
  file = {/Users/ag/Zotero/storage/55JYTX6Z/Lowndes et al. - 2017 - Our path to better science in less time using open.pdf;/Users/ag/Zotero/storage/R7ACNCHA/s41559-017-0160.html},
  journal = {Nature Ecology \& Evolution},
  language = {en},
  number = {6}
}

@article{ma2006,
  title = {Excess Commuting: {{A}} Critical Review},
  author = {Ma, K and Banister, D},
  year = {2006},
  volume = {26},
  pages = {749--767},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Transport Reviews},
  keywords = {ocp022019_REGION_template}
}

@article{markowetz2015,
  title = {Five Selfish Reasons to Work Reproducibly},
  author = {Markowetz, Florian},
  year = {2015},
  month = dec,
  volume = {16},
  pages = {274},
  issn = {1474-760X},
  doi = {10.1186/s13059-015-0850-7},
  file = {/Users/ag/Zotero/storage/RC383V7P/Markowetz - 2015 - Five selfish reasons to work reproducibly.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{markowetz2015a,
  title = {Five {{Selfish Reasons}} to {{Work Reproducibly}}},
  author = {Markowetz, Florian},
  year = {2015},
  month = dec,
  volume = {16},
  pages = {274},
  issn = {1474-760X},
  doi = {10.1186/s13059-015-0850-7},
  journal = {Genome Biology},
  language = {English},
  number = {1}
}

@article{markowetz2015b,
  title = {Five {{Selfish Reasons}} to {{Work Reproducibly}}},
  author = {Markowetz, Florian},
  year = {2015},
  month = dec,
  volume = {16},
  pages = {274},
  issn = {1474-760X},
  doi = {10.1186/s13059-015-0850-7},
  journal = {Genome Biology},
  language = {English},
  number = {1}
}

@article{markowetzFiveSelfishReasons2015,
  title = {Five {{Selfish Reasons}} to {{Work Reproducibly}}},
  author = {Markowetz, Florian},
  year = {2015},
  month = dec,
  volume = {16},
  pages = {274},
  issn = {1474-760X},
  doi = {10.1186/s13059-015-0850-7},
  journal = {Genome Biology},
  language = {English},
  number = {1}
}

@article{markowetzFiveSelfishReasons2015a,
  title = {Five {{Selfish Reasons}} to {{Work Reproducibly}}},
  author = {Markowetz, Florian},
  year = {2015},
  month = dec,
  volume = {16},
  pages = {274},
  issn = {1474-760X},
  doi = {10.1186/s13059-015-0850-7},
  journal = {Genome Biology},
  language = {English},
  number = {1}
}

@article{mccullough2008,
  title = {Do Economics Journal Archives Promote Replicable Research?},
  author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
  year = {2008},
  volume = {41},
  pages = {1406--1420},
  issn = {1540-5982},
  doi = {10.1111/j.1540-5982.2008.00509.x},
  abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
  annotation = {\_eprint: https://www.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5982.2008.00509.x},
  copyright = {\textcopyright{} Canadian Economics Association},
  file = {/Users/ag/Zotero/storage/IG3UBBR6/McCullough et al. - 2008 - Do economics journal archives promote replicable r.pdf;/Users/ag/Zotero/storage/8RVGH4TY/j.1540-5982.2008.00509.html},
  journal = {Canadian Journal of Economics/Revue canadienne d'\'economique},
  keywords = {B40,C80},
  language = {en},
  number = {4}
}

@article{mccullough2008a,
  title = {Do {{Economics Journal Archives Promote Replicable Research}}?},
  author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
  year = {2008},
  volume = {41},
  pages = {1406--1420},
  issn = {1540-5982},
  doi = {10.1111/j.1540-5982.2008.00509.x},
  abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
  copyright = {\textcopyright{} Canadian Economics Association},
  journal = {Canadian Journal of Economics/Revue canadienne d'\'economique},
  keywords = {done},
  language = {English},
  number = {4}
}

@article{mccullough2008b,
  title = {Do {{Economics Journal Archives Promote Replicable Research}}?},
  author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
  year = {2008},
  volume = {41},
  pages = {1406--1420},
  issn = {1540-5982},
  doi = {10.1111/j.1540-5982.2008.00509.x},
  abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
  copyright = {\textcopyright{} Canadian Economics Association},
  journal = {Canadian Journal of Economics/Revue canadienne d'\'economique},
  keywords = {done},
  language = {English},
  number = {4}
}

@article{mcculloughEconomicsJournalArchives2008,
  title = {Do {{Economics Journal Archives Promote Replicable Research}}?},
  author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
  year = {2008},
  volume = {41},
  pages = {1406--1420},
  issn = {1540-5982},
  doi = {10.1111/j.1540-5982.2008.00509.x},
  abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
  copyright = {\textcopyright{} Canadian Economics Association},
  journal = {Canadian Journal of Economics/Revue canadienne d'\'economique},
  keywords = {done},
  language = {English},
  number = {4}
}

@article{mcculloughEconomicsJournalArchives2008a,
  title = {Do {{Economics Journal Archives Promote Replicable Research}}?},
  author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
  year = {2008},
  volume = {41},
  pages = {1406--1420},
  issn = {1540-5982},
  doi = {10.1111/j.1540-5982.2008.00509.x},
  abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
  copyright = {\textcopyright{} Canadian Economics Association},
  journal = {Canadian Journal of Economics/Revue canadienne d'\'economique},
  keywords = {done},
  language = {English},
  number = {4}
}

@article{mcmillen2003,
  title = {Spatial {{Autocorrelation}} or Model Misspecification?},
  author = {McMillen, DP},
  year = {2003},
  volume = {26},
  pages = {208--217},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {International Regional Science Review},
  number = {2}
}

@article{mcnutt2014,
  title = {Reproducibility},
  author = {McNutt, Marcia},
  year = {2014},
  volume = {343},
  pages = {229--229},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1250475},
  eprint = {https://science.sciencemag.org/content/343/6168/229.full.pdf},
  journal = {Science},
  number = {6168}
}

@article{mcnutt2014a,
  title = {Reproducibility},
  author = {McNutt, Marcia},
  year = {2014},
  volume = {343},
  pages = {229--229},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1250475},
  journal = {Science},
  keywords = {done},
  number = {6168}
}

@article{mcnutt2014b,
  title = {Reproducibility},
  author = {McNutt, Marcia},
  year = {2014},
  volume = {343},
  pages = {229--229},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1250475},
  journal = {Science},
  keywords = {done},
  number = {6168}
}

@article{McNutt229,
  title = {Reproducibility},
  author = {McNutt, Marcia},
  year = {2014},
  volume = {343},
  pages = {229--229},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1250475},
  eprint = {https://science.sciencemag.org/content/343/6168/229.full.pdf},
  journal = {Science},
  number = {6168}
}

@article{mcnuttReproducibility2014,
  title = {Reproducibility},
  author = {McNutt, Marcia},
  year = {2014},
  volume = {343},
  pages = {229--229},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1250475},
  journal = {Science},
  keywords = {done},
  number = {6168}
}

@article{mcnuttReproducibility2014a,
  title = {Reproducibility},
  author = {McNutt, Marcia},
  year = {2014},
  volume = {343},
  pages = {229--229},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1250475},
  journal = {Science},
  keywords = {done},
  number = {6168}
}

@techreport{mora2010,
  title = {Entropy-Based Segregation Indices},
  author = {Mora, Ricardo and {Ruiz-Castillo}, Javier},
  year = {2010},
  month = jun,
  institution = {{Universidad Carlos III de Madrid. Departamento de Econom\'ia}},
  abstract = {Recent research has shown that two entropy-based segregation indices possess an appealing mixture of basic and subsidiary but useful properties. It would appear that the only fundamental difference between the mutual information, or M index, and the Entropy, Information or H index, is that the second is a normalized version of the first. This paper introduces another normalized index in that famiy, the H* index that, contrary to what is often asserted in the literature, is the normalized entropy index that captures the notion of segregation as departures from evenness. More importantly, the paper shows that applied researchers may do better using the M index than using either H or H* in two dircunstances: (i) if they are interested in the decomposability of segregation measures for any partition of organizational units into larger clusters and of demographic groups into supergroups, and (ii) if they are interested in the invariance properties of segregation measures to changes in the marginal distributions by demographic groups and by organizational units},
  file = {/Users/ag/Zotero/storage/7HZ3DE86/Mora and Ruiz-Castillo - 2010 - Entropy-based segregation indices.pdf;/Users/ag/Zotero/storage/ULWPL6JT/we1012.html},
  keywords = {Axiomatic properties},
  language = {en},
  number = {we1012}
}

@article{nordvik2019,
  title = {Capitalization of Neighbourhood Diversity and Segregation},
  author = {Nordvik, Viggo and Osland, Liv and Thorsen, Inge and Thorsen, Ingrid Sandvig},
  year = {2019},
  month = jul,
  volume = {51},
  pages = {1775--1799},
  issn = {0308-518X},
  doi = {10.1177/0308518X19861108},
  journal = {Environment and Planning A: Economy and Space},
  number = {8}
}

@article{nosek2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  file = {/Users/ag/Zotero/storage/8PJSTSZH/Nosek et al. - 2015 - Promoting an open research culture.pdf},
  journal = {Science},
  language = {en},
  number = {6242}
}

@article{nosek2015a,
  title = {Promoting an {{Open Research Culture}}},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6242}
}

@article{nosek2015b,
  title = {Promoting an {{Open Research Culture}}},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6242}
}

@article{nosek2015c,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Nosek, Brian A. and {et al}},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@article{nosek2015d,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Nosek, Brian A. and {et al}},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@article{nosekEstimatingReproducibilityPsychological2015,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Nosek, Brian A. and {et al}},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@article{nosekEstimatingReproducibilityPsychological2015a,
  title = {Estimating the {{Reproducibility}} of {{Psychological Science}}},
  author = {Nosek, Brian A. and {et al}},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  journal = {Science},
  keywords = {done},
  number = {6251}
}

@article{nosekPromotingOpenResearch2015,
  title = {Promoting an {{Open Research Culture}}},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6242}
}

@article{nosekPromotingOpenResearch2015a,
  title = {Promoting an {{Open Research Culture}}},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6242}
}

@article{nust2017,
  title = {Opening the {{Publication Process}} with {{Executable Research Compendia}}},
  author = {N{\"u}st, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, J{\"o}rg},
  year = {2017},
  month = jan,
  volume = {23},
  issn = {1082-9873},
  doi = {10.1045/january2017-nuest},
  journal = {D-Lib Magazine},
  language = {en},
  number = {1/2}
}

@article{nust2017a,
  title = {Opening the {{Publication Process}} with {{Executable Research Compendia}}},
  author = {N{\"u}st, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, J{\"o}rg},
  year = {2017},
  month = jan,
  volume = {23},
  issn = {1082-9873},
  doi = {10.1045/january2017-nuest},
  journal = {D-Lib Magazine},
  language = {English},
  number = {1/2}
}

@article{nust2017b,
  title = {Opening the {{Publication Process}} with {{Executable Research Compendia}}},
  author = {N{\"u}st, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, J{\"o}rg},
  year = {2017},
  month = jan,
  volume = {23},
  issn = {1082-9873},
  doi = {10.1045/january2017-nuest},
  journal = {D-Lib Magazine},
  language = {English},
  number = {1/2}
}

@article{nustOpeningPublicationProcess2017,
  title = {Opening the {{Publication Process}} with {{Executable Research Compendia}}},
  author = {N{\"u}st, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, J{\"o}rg},
  year = {2017},
  month = jan,
  volume = {23},
  issn = {1082-9873},
  doi = {10.1045/january2017-nuest},
  journal = {D-Lib Magazine},
  language = {English},
  number = {1/2}
}

@article{nustOpeningPublicationProcess2017a,
  title = {Opening the {{Publication Process}} with {{Executable Research Compendia}}},
  author = {N{\"u}st, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, J{\"o}rg},
  year = {2017},
  month = jan,
  volume = {23},
  issn = {1082-9873},
  doi = {10.1045/january2017-nuest},
  journal = {D-Lib Magazine},
  language = {English},
  number = {1/2}
}

@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {Open Science Collaboration},
  year = {2015},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aac4716},
  abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  elocation-id = {aac4716},
  eprint = {https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
  journal = {Science},
  number = {6251}
}

@article{osland2007,
  title = {Housing Price Gradients in a Geography with One Dominating Center},
  author = {Osland, L and Thorsen, I and Gitlesen, JP},
  year = {2007},
  volume = {29},
  pages = {321--346},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Real Estate Research},
  keywords = {ocp022019_REGION_template},
  number = {3}
}

@article{osland2008,
  title = {Effects on Housing Prices of Urban Attraction and Labor Market Accessibility},
  author = {Osland, L and Thorsen, I},
  year = {2008},
  volume = {40},
  pages = {2490--2509},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Environment and Planning A},
  keywords = {ocp022019_REGION_template},
  number = {10}
}

@article{osland2010,
  title = {An Application of Spatial Econometrics in Relation to Hedonic House Price Modelling},
  author = {Osland, L},
  year = {2010},
  volume = {32},
  pages = {289--320},
  date-modified = {2014-02-28 12:24:08 +0000},
  journal = {Journal of Real Estate Research},
  keywords = {ocp022019_REGION_template}
}

@article{osland2012,
  title = {Housing Prices and Multiple Employment Nodes: {{Is}} the Relationship Nonmonotonic?},
  author = {Osland, L and Pryce, G},
  year = {2012},
  volume = {27},
  pages = {1182--1208},
  date-modified = {2014-02-28 12:24:08 +0000},
  journal = {Housing Studies},
  keywords = {ocp022019_REGION_template}
}

@article{osland2013,
  title = {Spatial Impacts, Local Labour Market Characteristics and Housing Prices},
  author = {Osland, L and Thorsen, I},
  year = {2013},
  volume = {10},
  pages = {2063--2083},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Urban Studies},
  keywords = {ocp022019_REGION_template},
  number = {50}
}

@article{pace1998,
  title = {Appraisal Using Generalized Additive Models},
  author = {Pace, RK},
  year = {1998},
  volume = {15},
  pages = {77--99},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Real Estate Research},
  keywords = {ocp022019_REGION_template}
}

@book{PandocPandocUser,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}}
}

@book{PandocPandocUsera,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}}
}

@article{peng2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  volume = {334},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  file = {/Users/ag/Zotero/storage/TNKC5BRR/Peng - 2011 - Reproducible Research in Computational Science.pdf;/Users/ag/Zotero/storage/N5V26MV8/1226.html},
  journal = {Science},
  language = {en},
  number = {6060},
  pmid = {22144613}
}

@article{peng2011a,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  volume = {334},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6060},
  pmid = {22144613}
}

@article{peng2011b,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  volume = {334},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6060},
  pmid = {22144613}
}

@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  volume = {334},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6060},
  pmid = {22144613}
}

@article{pengReproducibleResearchComputational2011a,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  volume = {334},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  journal = {Science},
  keywords = {done},
  language = {English},
  number = {6060},
  pmid = {22144613}
}

@article{plaut1998,
  title = {Endogenous Identification of Multiple Housing Price Centers in Metropolitan Areas},
  author = {Plaut, PO and Plaut, SE},
  year = {1998},
  volume = {7},
  pages = {193--217},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Housing Economics},
  keywords = {ocp022019_REGION_template}
}

@article{portnov2010,
  title = {On the Suitability of Income Inequality Measures for Regional Analysis: {{Some}} Evidence from Simulation Analysis and Bootstrapping Tests},
  shorttitle = {On the Suitability of Income Inequality Measures for Regional Analysis},
  author = {Portnov, Boris A. and Felsenstein, Daniel},
  year = {2010},
  month = dec,
  volume = {44},
  pages = {212--219},
  issn = {0038-0121},
  doi = {10.1016/j.seps.2010.04.002},
  abstract = {The paper looks at the sensitivity of commonly used income inequality measures to changes in the ranking, size and number of regions into which a country is divided. During the analysis, several test distributions of populations and incomes are compared with a `reference' distribution, characterized by an even distribution of population across regional subdivisions. Random permutation tests are also run to determine whether inequality measures commonly used in regional analysis produce meaningful estimates when applied to regions of different population size. The results show that only the population weighted coefficient of variation (Williamson's index) and population-weighted Gini coefficient may be considered sufficiently reliable inequality measures, when applied to countries with a small number of regions and with varying population sizes.},
  file = {/Users/ag/Zotero/storage/Y2JT8UWY/Portnov and Felsenstein - 2010 - On the suitability of income inequality measures f.pdf;/Users/ag/Zotero/storage/6NIQPMMI/S0038012110000145.html},
  journal = {Socio-Economic Planning Sciences},
  keywords = {Bootstrapping,Inequality measures,Random permutation tests,Regions},
  number = {4}
}

@article{ram2013,
  title = {Git Can Facilitate Greater Reproducibility and Increased Transparency in Science},
  author = {Ram, Karthik},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  file = {/Users/ag/Zotero/storage/N2EZSANN/Ram - 2013 - Git can facilitate greater reproducibility and inc.pdf;/Users/ag/Zotero/storage/FFMA66JW/1751-0473-8-7.html},
  journal = {Source Code for Biology and Medicine},
  number = {1}
}

@article{ram2013a,
  title = {Git {{Can Facilitate Greater Reproducibility}} and {{Increased Transparency}} in {{Science}}},
  author = {Ram, Karthik},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  journal = {Source Code for Biology and Medicine},
  number = {1}
}

@article{ram2013b,
  title = {Git {{Can Facilitate Greater Reproducibility}} and {{Increased Transparency}} in {{Science}}},
  author = {Ram, Karthik},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  journal = {Source Code for Biology and Medicine},
  number = {1}
}

@article{ramGitCanFacilitate2013,
  title = {Git {{Can Facilitate Greater Reproducibility}} and {{Increased Transparency}} in {{Science}}},
  author = {Ram, Karthik},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  journal = {Source Code for Biology and Medicine},
  number = {1}
}

@article{ramGitCanFacilitate2013a,
  title = {Git {{Can Facilitate Greater Reproducibility}} and {{Increased Transparency}} in {{Science}}},
  author = {Ram, Karthik},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  journal = {Source Code for Biology and Medicine},
  number = {1}
}

@book{ramsey,
  title = {Noweb {{Home Page}}},
  author = {Ramsey, Norman},
  keywords = {done}
}

@article{ramsey1969,
  title = {Tests for Specification Errors in Classical Linear Least Squares Regression Analysis},
  author = {Ramsey, JB},
  year = {1969},
  volume = {31},
  pages = {350--371},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Royal Statistical Society B},
  keywords = {ocp022019_REGION_template},
  number = {2}
}

@book{ramseya,
  title = {Noweb {{Home Page}}},
  author = {Ramsey, Norman},
  keywords = {done}
}

@book{ramseyNowebHomePage,
  title = {Noweb {{Home Page}}},
  author = {Ramsey, Norman},
  keywords = {done}
}

@book{ramseyNowebHomePagea,
  title = {Noweb {{Home Page}}},
  author = {Ramsey, Norman},
  keywords = {done}
}

@manual{rcoreteam2020,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}},
  type = {Manual}
}

@book{rcoreteam2020a,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@book{rcoreteam2020b,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2020},
  publisher = {{R Foundation for Statistical Computing}},
  address = {{Vienna, Austria}},
  keywords = {done}
}

@book{rcoreteam2020c,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2020},
  publisher = {{R Foundation for Statistical Computing}},
  address = {{Vienna, Austria}},
  keywords = {done}
}

@book{rcoreteamLanguageEnvironmentStatistical2020,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2020},
  publisher = {{R Foundation for Statistical Computing}},
  address = {{Vienna, Austria}},
  keywords = {done}
}

@book{rcoreteamLanguageEnvironmentStatistical2020a,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2020},
  publisher = {{R Foundation for Statistical Computing}},
  address = {{Vienna, Austria}},
  keywords = {done}
}

@article{rendell2019,
  title = {Opinion | {{I}} like {{Elizabeth Warren}}. {{Too}} Bad She's a Hypocrite.},
  author = {Rendell, Ed},
  year = {2019-09-11T06:53-500},
  issn = {0190-8286},
  abstract = {Her campaign finance promises are hypocritical.},
  chapter = {Opinions},
  file = {/Users/ag/Zotero/storage/28CY24MP/409d5ed0-d4d3-11e9-86ac-0f250cc91758_story.html},
  journal = {Washington Post},
  language = {en-US}
}

@manual{revelle2020,
  title = {Psych: {{Procedures}} for Psychological, Psychometric, and Personality Research},
  author = {Revelle, William},
  year = {2020},
  address = {{Evanston, Illinois}},
  organization = {{Northwestern University}},
  type = {Manual}
}

@book{riederer,
  title = {R {{Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
  file = {/Users/ag/Zotero/storage/XRX2VJVT/rmarkdown-cookbook.html}
}

@book{riedererMarkdownCookbook,
  title = {R {{Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Yihui Xie, Emily},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
  keywords = {done}
}

@book{riedererMarkdownCookbooka,
  title = {R {{Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Yihui Xie, Emily},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
  keywords = {done}
}

@misc{rognsvag2019,
  title = {{Revisorforeininga: \textendash{} Veldig alvorleg sak}},
  shorttitle = {{Revisorforeininga}},
  author = {Rognsv{\aa}g, Silje},
  year = {2019},
  month = apr,
  abstract = {Per Hanstad i Den norske revisorforeininga meiner at det er lite truverdig at Ap-politikar Liadal skuldar p\aa{} rot i reiserekningssaka.},
  file = {/Users/ag/Zotero/storage/R6AN9F7A/revisorforeininga_-_-veldig-alvorleg-sak-1.html},
  howpublished = {https://www.nrk.no/norge/revisorforeininga\_-\_-veldig-alvorleg-sak-1.14510594},
  journal = {NRK},
  language = {nn-NO}
}

@inproceedings{rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results.},
  author = {Rosenthal, R.},
  year = {1979},
  file = {/Users/ag/Zotero/storage/EL2FFM8Y/Rosenthal1979PsychBulletin.pdf}
}

@inproceedings{rosenthal1979a,
  title = {The {{File Drawer Problem}} and {{Tolerance}} for {{Null Results}}.},
  author = {Rosenthal, R.},
  year = {1979},
  volume = {86},
  pages = {638--641},
  publisher = {{Psychological Bulletin}},
  keywords = {done}
}

@inproceedings{rosenthal1979b,
  title = {The {{File Drawer Problem}} and {{Tolerance}} for {{Null Results}}.},
  author = {Rosenthal, R.},
  year = {1979},
  volume = {86},
  pages = {638--641},
  publisher = {{Psychological Bulletin}},
  keywords = {done}
}

@inproceedings{rosenthalFileDrawerProblem1979,
  title = {The {{File Drawer Problem}} and {{Tolerance}} for {{Null Results}}.},
  author = {Rosenthal, R.},
  year = {1979},
  volume = {86},
  pages = {638--641},
  publisher = {{Psychological Bulletin}},
  keywords = {done}
}

@inproceedings{rosenthalFileDrawerProblem1979a,
  title = {The {{File Drawer Problem}} and {{Tolerance}} for {{Null Results}}.},
  author = {Rosenthal, R.},
  year = {1979},
  volume = {86},
  pages = {638--641},
  publisher = {{Psychological Bulletin}},
  keywords = {done}
}

@article{rossi,
  title = {Measuring the Tracking Error  of Exchange Traded Funds:  An Unobserved Components Approach},
  author = {Rossi, Giuliano De},
  pages = {36},
  file = {/Users/ag/Zotero/storage/3SSAPAIW/Rossi - Measuring the tracking error  of exchange traded f.pdf},
  language = {en}
}

@book{rstudioteam2020,
  title = {{{RStudio}}: {{Integrated}} Development Environment for r},
  author = {{RStudio Team}},
  year = {2020},
  address = {{Boston, MA}},
  organization = {{RStudio, PBC.}}
}

@book{rstudioteam2020a,
  title = {{{RStudio}}: {{Integrated Development Environment}} for r},
  author = {{RStudio Team}},
  year = {2020},
  publisher = {{RStudio, PBC.}},
  address = {{Boston, MA}}
}

@book{rstudioteam2020b,
  title = {{{RStudio}}: {{Integrated Development Environment}} for r},
  author = {{RStudio Team}},
  year = {2020},
  publisher = {{RStudio, PBC.}},
  address = {{Boston, MA}}
}

@book{rstudioteamRStudioIntegratedDevelopment2020,
  title = {{{RStudio}}: {{Integrated Development Environment}} for r},
  author = {{RStudio Team}},
  year = {2020},
  publisher = {{RStudio, PBC.}},
  address = {{Boston, MA}}
}

@book{rstudioteamRStudioIntegratedDevelopment2020a,
  title = {{{RStudio}}: {{Integrated Development Environment}} for r},
  author = {{RStudio Team}},
  year = {2020},
  publisher = {{RStudio, PBC.}},
  address = {{Boston, MA}}
}

@article{schwab1995,
  title = {Reproducible {{Electronic Documents}}},
  author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
  year = {1995},
  pages = {14},
  abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
  keywords = {done},
  language = {English}
}

@article{schwab1995a,
  title = {Reproducible {{Electronic Documents}}},
  author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
  year = {1995},
  pages = {14},
  abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
  keywords = {done},
  language = {English}
}

@article{schwabReproducibleElectronicDocuments1995,
  title = {Reproducible {{Electronic Documents}}},
  author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
  year = {1995},
  pages = {14},
  abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
  keywords = {done},
  language = {English}
}

@article{schwabReproducibleElectronicDocuments1995a,
  title = {Reproducible {{Electronic Documents}}},
  author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
  year = {1995},
  pages = {14},
  abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
  keywords = {done},
  language = {English}
}

@article{simmons2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  volume = {22},
  pages = {1359--1366},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  eprint = {https://doi.org/10.1177/0956797611417632},
  journal = {Psychological Science},
  number = {11}
}

@article{simmons2011a,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  volume = {22},
  pages = {1359--1366},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\$\textbackslash leq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  keywords = {done},
  number = {11},
  pmid = {22006061}
}

@article{simmons2011b,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  volume = {22},
  pages = {1359--1366},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\$\textbackslash leq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  keywords = {done},
  number = {11},
  pmid = {22006061}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  volume = {22},
  pages = {1359--1366},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\$\textbackslash leq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  keywords = {done},
  number = {11},
  pmid = {22006061}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011a,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  volume = {22},
  pages = {1359--1366},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (\$\textbackslash leq\$ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  journal = {Psychological Science},
  keywords = {done},
  number = {11},
  pmid = {22006061}
}

@article{smith2008,
  title = {Moving from an {{Efficient}} to a {{Behavioral Market Hypothesis}}},
  author = {Smith, Donald J.},
  year = {2008},
  volume = {9},
  pages = {51--52},
  issn = {1542-7560},
  doi = {10.1080/15427560802093589},
  abstract = {Behavioral finance typically is introduced in investments courses in the context of the efficient markets hypothesis (see, e.g., the widely used textbooks by Bodie, Kane, and Marcus [2002] and Reilly and Brown [2003] ). This note offers a diagrammatic approach to ``position'' visually behavioral finance between the information available to market participants and their investment decisions.},
  journal = {Journal of Behavioral Finance},
  keywords = {Business;},
  number = {2}
}

@book{tierney,
  title = {{{RMarkdown}} for {{Scientists}}},
  author = {Tierney, Nicholas},
  abstract = {A book created for a 3 hour workshop on rmarkdown},
  file = {/Users/ag/Zotero/storage/6G66Z9WU/rmd4sci.njtierney.com.html}
}

@book{tierneya,
  title = {13 {{Citing Articles}} \& {{Bibliography Styles}} | {{RMarkdown}} for {{Scientists}}},
  author = {Tierney, Nicholas},
  abstract = {A book created for a 3 hour workshop on rmarkdown},
  file = {/Users/ag/Zotero/storage/WD43YPUN/citing-articles-bibliography-styles.html}
}

@book{tierneyb,
  title = {{{RMarkdown}} for {{Scientists}}},
  author = {Tierney, Nicholas},
  abstract = {A book created for a 3 hour workshop on rmarkdown},
  keywords = {done}
}

@book{tierneyc,
  title = {{{RMarkdown}} for {{Scientists}}},
  author = {Tierney, Nicholas},
  abstract = {A book created for a 3 hour workshop on rmarkdown},
  keywords = {done}
}

@book{tierneyRMarkdownScientists,
  title = {{{RMarkdown}} for {{Scientists}}},
  author = {Tierney, Nicholas},
  abstract = {A book created for a 3 hour workshop on rmarkdown},
  keywords = {done}
}

@book{tierneyRMarkdownScientistsa,
  title = {{{RMarkdown}} for {{Scientists}}},
  author = {Tierney, Nicholas},
  abstract = {A book created for a 3 hour workshop on rmarkdown},
  keywords = {done}
}

@book{vagane2011,
  title = {Den Nasjonale Reisevaneunders\o kelsen - 2009},
  author = {V{\aa}gane, L and Brechan, I and Hjorthol, R},
  year = {2011},
  publisher = {{The Institute for Transport Economics}},
  address = {{T\O I Report 1130}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@article{vaona2009,
  title = {Spatial Autocorrelation or Model Misspecification? {{The}} Help from {{RESET}} and the Curse of Small Samples.},
  author = {Vaona, A},
  year = {2009},
  volume = {2},
  pages = {53--59},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Letters of Spatial Resource Sciences}
}

@article{waddell1993,
  title = {Residential Property Values in a Multinodal Urban Area: {{New}} Evidence on the Implicit Price of Location},
  author = {Waddell, P and Berry, BJL and Hoch, I},
  year = {1993},
  volume = {7},
  pages = {117--141},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of real estate finance and economics},
  keywords = {ocp022019_REGION_template}
}

@manual{waring2020,
  title = {Skimr: {{Compact}} and Flexible Summaries of Data},
  author = {Waring, Elin and Quinn, Michael and McNamara, Amelia and {de la Rubia}, Eduardo Arino and Zhu, Hao and Ellis, Shannon},
  year = {2020},
  type = {Manual}
}

@book{wickham,
  title = {The Tidyverse Style Guide},
  author = {Wickham, Hadley},
  abstract = {The tidyverse style guide},
  file = {/Users/ag/Zotero/storage/MEMY2VNN/style.tidyverse.org.html}
}

@article{wickham2014,
  title = {Tidy Data},
  author = {Wickham, Hadley},
  year = {2014},
  month = sep,
  volume = {14},
  doi = {10.18637/jss.v059.i10},
  journal = {The American Statistician}
}

@book{wickham2016,
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  shorttitle = {R for Data Science},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  pages = {XXV, 492},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-1039-9},
  keywords = {Computer programs; Information visualization,Computer programs; R (Computer program language); Statistiske metoder; R-språk; programmering; R; statistikk; data science; datavitenskap,Data mining},
  language = {eng},
  lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC}
}

@book{wickham2016a,
  title = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  shorttitle = {R for {{Data Science}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-1039-9},
  language = {English},
  lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC}
}

@book{wickham2016b,
  title = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  shorttitle = {R for {{Data Science}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-1039-9},
  language = {English},
  lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC}
}

@book{wickhamDataScienceImport2016,
  title = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  shorttitle = {R for {{Data Science}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-1039-9},
  language = {English},
  lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC}
}

@book{wickhamDataScienceImport2016a,
  title = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  shorttitle = {R for {{Data Science}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4919-1039-9},
  language = {English},
  lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC}
}

@article{wikipedia2020,
  title = {Meta-{{Analysis}}},
  author = {{wikipedia}},
  year = {2020},
  month = aug,
  abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal = {Wikipedia},
  keywords = {done},
  language = {English}
}

@article{wikipedia2020a,
  title = {Meta-{{Analysis}}},
  author = {{wikipedia}},
  year = {2020},
  month = aug,
  abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal = {Wikipedia},
  keywords = {done},
  language = {English}
}

@article{wikipediaMetaAnalysis2020,
  title = {Meta-{{Analysis}}},
  author = {{wikipedia}},
  year = {2020},
  month = aug,
  abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal = {Wikipedia},
  keywords = {done},
  language = {English}
}

@article{wikipediaMetaAnalysis2020a,
  title = {Meta-{{Analysis}}},
  author = {{wikipedia}},
  year = {2020},
  month = aug,
  abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews. A meta-analysis is a secondary source.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal = {Wikipedia},
  keywords = {done},
  language = {English}
}

@article{wilhelmsson2000,
  title = {The Impact of Traffic Noise on the Values of Single-Family Houses},
  author = {Wilhelmsson, M},
  year = {2000},
  volume = {43},
  pages = {799--815},
  date-modified = {2012-02-28 12:24:08 +0000},
  journal = {Journal of Environmental Planning and Management},
  number = {6}
}

@incollection{wood2006,
  title = {Generalized Additive Models},
  booktitle = {An Introduction with {{R}}},
  author = {Wood, SN},
  year = {2006},
  publisher = {{Texts in Statistical Science}},
  address = {{Chapman \& Hall/CRC, New YorkTexts in Statistical Science, Chapman \& Hall/CRC, New York}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@book{wooldridge2003,
  title = {Introductory Econometrics. {{A}} Modern Approach.},
  author = {Wooldridge, JM},
  year = {2003},
  publisher = {{Thomson. South Western}},
  address = {{USA}},
  date-modified = {2012-02-28 12:24:08 +0000},
  keywords = {ocp022019_REGION_template}
}

@article{xiao2016,
  title = {Urban Configuration, Accessibility, and Property Prices: A Case Study of {{Cardiff}}, {{Wales}}},
  author = {Xiao, Y and Orford, S and Webster, C J},
  year = {2016},
  volume = {43},
  pages = {108--129},
  date-added = {2015-10-20 11:35:51 +0000},
  date-modified = {2015-10-20 11:36:52 +0000},
  journal = {Environment and Planning B: Planning and Design},
  keywords = {ocp022019_REGION_template}
}

@book{xie,
  title = {R {{Markdown Cookbook}}},
  author = {Xie, Yihui and Dervieux, Christophe and Riederer, Emily},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
  file = {/Users/ag/Zotero/storage/R57E5JD3/rmarkdown-cookbook.html}
}

@incollection{xie2014,
  title = {Knitr: {{A}} Comprehensive Tool for Reproducible Research in {{R}}},
  booktitle = {Implementing Reproducible Computational Research},
  author = {Xie, Yihui},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}}
}

@incollection{xie2014a,
  title = {Knitr: {{A}} Comprehensive Tool for Reproducible Research in {{R}}},
  booktitle = {Implementing Reproducible Computational Research},
  author = {Xie, Yihui},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}}
}

@incollection{xie2014b,
  title = {Knitr: {{A Comprehensive Tool}} for {{Reproducible Research}} in {{R}}},
  booktitle = {Implementing {{Reproducible Computational Research}}},
  author = {Xie, Yihui},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}},
  keywords = {done}
}

@incollection{xie2014c,
  title = {Knitr: {{A Comprehensive Tool}} for {{Reproducible Research}} in {{R}}},
  booktitle = {Implementing {{Reproducible Computational Research}}},
  author = {Xie, Yihui},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}},
  keywords = {done}
}

@book{xie2015,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{xie2015a,
  title = {Dynamic Documents with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{xie2015b,
  title = {Dynamic {{Documents}} with {{R}} and {{Knitr}}},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@book{xie2015c,
  title = {Dynamic {{Documents}} with {{R}} and {{Knitr}}},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@book{xie2018,
  title = {R Markdown: {{The}} Definitive Guide},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@book{xie2018a,
  title = {R {{Markdown}}: {{The Definitive Guide}}},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@book{xie2018b,
  title = {R {{Markdown}}: {{The Definitive Guide}}},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@manual{xie2020,
  title = {Knitr: {{A}} General-Purpose Package for Dynamic Report Generation in r},
  author = {Xie, Yihui},
  year = {2020},
  type = {Manual}
}

@book{xie2020a,
  title = {Knitr: {{A}} General-Purpose Package for Dynamic Report Generation in r},
  author = {Xie, Yihui},
  year = {2020}
}

@book{xie2020b,
  title = {Knitr: {{A General}}-{{Purpose Package}} for {{Dynamic Report Generation}} in r},
  author = {Xie, Yihui},
  year = {2020},
  keywords = {done},
  type = {Manual}
}

@book{xie2020c,
  title = {Knitr: {{A General}}-{{Purpose Package}} for {{Dynamic Report Generation}} in r},
  author = {Xie, Yihui},
  year = {2020},
  keywords = {done},
  type = {Manual}
}

@book{xieDynamicDocumentsKnitr2015,
  title = {Dynamic {{Documents}} with {{R}} and {{Knitr}}},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@book{xieDynamicDocumentsKnitr2015a,
  title = {Dynamic {{Documents}} with {{R}} and {{Knitr}}},
  author = {Xie, Yihui},
  year = {2015},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@incollection{xieKnitrComprehensiveTool2014,
  title = {Knitr: {{A Comprehensive Tool}} for {{Reproducible Research}} in {{R}}},
  booktitle = {Implementing {{Reproducible Computational Research}}},
  author = {Xie, Yihui},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}},
  keywords = {done}
}

@incollection{xieKnitrComprehensiveTool2014a,
  title = {Knitr: {{A Comprehensive Tool}} for {{Reproducible Research}} in {{R}}},
  booktitle = {Implementing {{Reproducible Computational Research}}},
  author = {Xie, Yihui},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}},
  keywords = {done}
}

@book{xieKnitrGeneralPurposePackage2020,
  title = {Knitr: {{A General}}-{{Purpose Package}} for {{Dynamic Report Generation}} in r},
  author = {Xie, Yihui},
  year = {2020},
  keywords = {done},
  type = {Manual}
}

@book{xieKnitrGeneralPurposePackage2020a,
  title = {Knitr: {{A General}}-{{Purpose Package}} for {{Dynamic Report Generation}} in r},
  author = {Xie, Yihui},
  year = {2020},
  keywords = {done},
  type = {Manual}
}

@book{xieMarkdownDefinitiveGuide2018,
  title = {R {{Markdown}}: {{The Definitive Guide}}},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@book{xieMarkdownDefinitiveGuide2018a,
  title = {R {{Markdown}}: {{The Definitive Guide}}},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}},
  keywords = {done}
}

@manual{yoshida2020,
  title = {Tableone: {{Create}} 'table 1' to Describe Baseline Characteristics with or without Propensity Score Weights},
  author = {Yoshida, Kazuki and Bartel, Alexander},
  year = {2020},
  type = {Manual}
}

@article{young2008,
  title = {Why {{Current Publication Practices May Distort Science}}},
  author = {Young, Neal S and Ioannidis, John P. A and {Al-Ubaydli}, Omar},
  year = {2008},
  month = oct,
  volume = {5},
  pages = {e201},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0050201},
  abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The ``winner's curse,'' a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
  file = {/Users/ag/Zotero/storage/GN54LYE7/Young et al. - 2008 - Why Current Publication Practices May Distort Scie.pdf},
  journal = {PLoS Medicine},
  language = {en},
  number = {10}
}

@article{young2008a,
  title = {Why {{Current Publication Practices May Distort Science}}},
  author = {Young, Neal S and Ioannidis, John P. A and {Al-Ubaydli}, Omar},
  year = {2008},
  month = oct,
  volume = {5},
  pages = {e201},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0050201},
  abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The ``winner's curse,'' a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
  journal = {PLoS Medicine},
  keywords = {done},
  language = {English},
  number = {10}
}

@article{young2008b,
  title = {Why {{Current Publication Practices May Distort Science}}},
  author = {Young, Neal S and Ioannidis, John P. A and {Al-Ubaydli}, Omar},
  year = {2008},
  month = oct,
  volume = {5},
  pages = {e201},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0050201},
  abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The ``winner's curse,'' a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
  journal = {PLoS Medicine},
  keywords = {done},
  language = {English},
  number = {10}
}

@article{youngWhyCurrentPublication2008,
  title = {Why {{Current Publication Practices May Distort Science}}},
  author = {Young, Neal S and Ioannidis, John P. A and {Al-Ubaydli}, Omar},
  year = {2008},
  month = oct,
  volume = {5},
  pages = {e201},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0050201},
  abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The ``winner's curse,'' a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
  journal = {PLoS Medicine},
  keywords = {done},
  language = {English},
  number = {10}
}

@article{youngWhyCurrentPublication2008a,
  title = {Why {{Current Publication Practices May Distort Science}}},
  author = {Young, Neal S and Ioannidis, John P. A and {Al-Ubaydli}, Omar},
  year = {2008},
  month = oct,
  volume = {5},
  pages = {e201},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0050201},
  abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The ``winner's curse,'' a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
  journal = {PLoS Medicine},
  keywords = {done},
  language = {English},
  number = {10}
}

@misc{zotero-101,
  title = {Opinion | {{I}} like {{Elizabeth Warren}}. {{Too}} Bad She's a Hypocrite.},
  abstract = {Her campaign finance promises are hypocritical.},
  file = {/Users/ag/Zotero/storage/N94GZ9GG/409d5ed0-d4d3-11e9-86ac-0f250cc91758_story.html},
  howpublished = {https://www.washingtonpost.com/opinions/i-like-elizabeth-warren-too-bad-shes-a-hypocrite/2019/09/11/409d5ed0-d4d3-11e9-86ac-0f250cc91758\_story.html},
  journal = {Washington Post},
  language = {en}
}

@misc{zotero-102,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds readers' knowledge of and confidence in statistical modeling. Reflecting the need for even minor programming in today's model-based statistics, the book pushes readers to perform step-by-step calculations that are usual},
  file = {/Users/ag/Zotero/storage/G7W5DZQY/9781482253443.html},
  howpublished = {https://www.crcpress.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-Stan/McElreath/p/book/9781482253443},
  journal = {CRC Press},
  language = {en}
}

@misc{zotero-112,
  title = {Spgwr.Pdf},
  file = {/Users/ag/Zotero/storage/9KMZ67CZ/viewer.html},
  howpublished = {https://docs.google.com/viewer?url=https\%3A\%2F\%2Fcran.microsoft.com\%2Fweb\%2Fpackages\%2Fspgwr\%2Fspgwr.pdf\&pdf=true}
}

@misc{zotero-113,
  title = {{{spData}} Package | {{R Documentation}}},
  file = {/Users/ag/Zotero/storage/Q6ASPL3B/0.3.html},
  howpublished = {https://www.rdocumentation.org/packages/spData/versions/0.3.0}
}

@misc{zotero-116,
  title = {Online {{Data}} - {{Robert Shiller}}},
  file = {/Users/ag/Zotero/storage/SMYXLD9X/data.html},
  howpublished = {http://www.econ.yale.edu/\textasciitilde shiller/data.htm}
}

@misc{zotero-119,
  title = {Options {{Symbology Initiative}} ({{OSI}})},
  file = {/Users/ag/Zotero/storage/CFWPF55S/optionsymbologyinitiative.blogspot.no.html}
}

@misc{zotero-123,
  title = {Quick\_start\_guide [{{Zotero Documentation}}]},
  file = {/Users/ag/Zotero/storage/KAN6TZZ3/quick_start_guide.html},
  howpublished = {https://www.zotero.org/support/quick\_start\_guide}
}

@misc{zotero-210,
  title = {Norske B\o rshandlede Fond : En Kvantitativ Analyse Av Fondenes Egenskaper - {{CORE}}},
  file = {/Users/ag/Zotero/storage/X5T3CRB2/52072194.html},
  howpublished = {https://core.ac.uk/display/52072194}
}

@misc{zotero-217,
  howpublished = {https://bibsys-almaprimo.hosted.exlibrisgroup.com/primo-explore/fulldisplay?docid=BIBSYS\_ILS71542116430002201\&context=L\&vid=HIB\&lang=nn\_NO\&search\_scope=all\_blended\&adaptor=Local\%20Search\%20Engine\&isFrbr=true\&tab=default\_tab\&query=any,contains,r\%20for\%20data\%20science\&sortby=date\&facet=frbrgroupid,include,257567799\&offset=0}
}

@misc{zotero-218,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}},
  file = {/Users/ag/Zotero/storage/BKLFW3LT/MANUAL.html},
  howpublished = {https://pandoc.org/MANUAL.html}
}

@misc{zotero-324,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} (Nsf16137) | {{NSF}} - {{National Science Foundation}}},
  file = {/Users/ag/Zotero/storage/CARGSTUK/nsf16137.html},
  howpublished = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp}
}

@misc{zotero-349,
  title = {American {{Economic Association}}},
  file = {/Users/ag/Zotero/storage/MK7T7YML/data.html},
  howpublished = {https://www.aeaweb.org/journals/data}
}

@misc{zotero-72,
  title = {{Hva er nytt i Firefox}},
  abstract = {Hva er nytt i Firefox},
  file = {/Users/ag/Zotero/storage/8IJVJW8X/all.html},
  howpublished = {https://www.mozilla.org/nb-NO/firefox/80.0/whatsnew/all/},
  journal = {Mozilla},
  language = {nb-NO}
}

@book{zotero-729,
  title = {American {{Economic Association}}}
}

@book{zotero-730,
  title = {American {{Economic Association}}}
}

@book{zotero-747,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@book{zotero-748,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@book{zotero-749,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@book{zotero-750,
  title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} ({{Nsf16137}}) | {{NSF}} - {{National Science Foundation}}},
  keywords = {done}
}

@misc{zotero-78,
  title = {Introduction to Tableone},
  file = {/Users/ag/Zotero/storage/HWZA3CLC/introduction.html},
  howpublished = {https://cran.r-project.org/web/packages/tableone/vignettes/introduction.html}
}

@book{zotero-817,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}}
}

@book{zotero-818,
  title = {Pandoc - {{Pandoc User}}'s {{Guide}}}
}


