---
title: "Robust and Reliable Science"
author: "Arnsten Gjestland <br /><br />Department of Business Administration<br />Western Norway University of Applied Sciences <br />"
date: today
format:
  revealjs: 
    number-sections: true
    number-depth: 2
    slide-number: true
    title-slide-attributes:
      data-background-color: "#004357"
    toc: true
    toc-depth: 1
    preview-links: auto
    footer: <https://www.hvl.no> 
editor_options: 
  markdown: 
    wrap: 72
execute: 
  echo: true
  eval: true
---

## Reproducibility

Start of the editorial in *Science* January 2014

-   “Science advances on a foundation of trusted discoveries.
-   Reproducing an experiment is one important approach that scientists
    use to gain confidence in their conclusions.
-   Recently, the scientific community was shaken by reports that a
    troubling proportion of peer-reviewed preclinical studies are not
    reproducible”.[@mcnutt2014]
    -   advances on a foundation of **trusted** discoveries
    -   scientists try to reproduce results to **gain confidence** in
        conclusions
    -   many peer-reviewed studies **not reproducible**

## Replicability and Reproducibility

-   “Replication—The confirmation of results and conclusions from one
    study obtained independently in another—is considered the scientific
    gold standard.”[@jasny2011]
    -   Reproducibility is a *necessary*, but *not sufficient*,
        condition for Replicability.
    -   Should *Reproducibility* be a minimal requirement for
        publication? (@peng2011 discusses this with regard to
        Computational Science)

## NSF: Robust and Reliable Science

-   There seems to have been some confusion regarding the terms.

-   At least the NSF[^1] felt a need to define the following
    [@bollen2015]:

-   **“robust and reliable” science** refers to: *research that is*
    **reproducible**, **replicable**, *and* **generalizable.**

-   **Reproducibility** refers to: *the ability of a researcher to*
    **duplicate** *the results of a prior study using the same materials
    and procedures used by the original investigator.*

-   **Replicability** refers to: *the ability of a researcher to
    duplicate the results of a prior study if the same procedures are
    followed but* **new data** *are collected.*

-   **Generalizability** refers to: *whether the results of a study
    apply in other contexts or populations that differ from the
    originals.*

-   See also @zotero-380

-   Robust and reliable research is **the foundation of all scientific
    development** and progress, which depends critically on the ability
    of investigators to build on prior work.

## What does research reproducibility mean?

-   What does research reproducibility mean? [@goodman2016]
    -   **Methods reproducibility:** “Methods reproducibility refers to
        the provision of enough detail about study procedures and data
        so the same procedures could, in theory or in actuality, be
        exactly repeated. ”
    -   **Results reproducibility:** “Results reproducibility
        (previously described as replicability) refers to obtaining the
        same results from the conduct of an independent study whose
        procedures are as closely matched to the original experiment as
        possible.”
    -   **Robustness and generalizability:** “We briefly introduce these
        terms because they are sometimes used in lieu of the term
        reproducibility. Robustness refers to the stability of
        experimental conclusions to variations in either baseline
        assumptions or experimental procedures. It is somewhat related
        to the concept of generalizability (also known as
        transportability), which refers to the persistence of an effect
        in settings different from and outside of an experimental
        framework.”

## Publication bias

-   **Publication bias** refers to: the likelihood of a study being
    published based on the findings of the study.
    -   There are few published studies with a negative conclusion (can
        not reject H0),
        -   ex.: The treatment has no effect
    -   Worst case, Scientific journals are big collections of type 1
        error

## Problem: Type 1 error

-   Type 1 error: reject H0 when H0 is true
-   H0 is the conservative hypothesis, i.e. “no effect”
-   with $\alpha = 0,05$ we accept that in 1 of 20 cases we will reject
    H0 even if it\`s true
    -   i.e. we conclude that there is an effect when in reality there
        is none (all because we were unlucky, or perhaps lucky if we
        want to be published, and got an unfortunate sample)
-   So even if there are 100 similar studies that find no effect, there
    is a risk that the wrong one is what you will find in the literature
-   This is often called the “File Drawer Problem” after @rosenthal1979
    -   studies where H0 **can not be** rejected go to the filing
        cabinet
    -   studies where H0 **can be** rejected will be sendt to the
        journals

## Problem: Type 1 error

-   Perhaps the most costly error is a false positive, **the incorrect
    rejection of a null hypothesis**.
-   First, once they appear in the literature, false positives are
    particularly persistent.
-   Because null results have many possible causes, failures to
    replicate previous findings are never conclusive.
-   Furthermore, because it is uncommon for prestigious journals to
    publish null findings or exact replications, researchers have little
    incentive to even attempt them.
-   Second, false positives waste resources: They inspire investment in
    fruitless research programs and can lead to ineffective policy
    changes.
-   Finally, a field known for publishing false positives risks losing
    its credibility.[@simmons2011]

## Publication bias and meta-analysis

-   “More alarming is the general paucity in the literature of negative
    data. In some fields, almost all published studies show formally
    significant results so that statistical significance no longer
    appears discriminating” [@young2008]
    -   @young2008 analyses scientific information as an economic
        commodity
-   A **meta-analysis** is a statistical analysis that combines the
    results of multiple scientific studies.[@wikipedia2020]
-   If there is a publication bias this will also influence
    meta-analysis'
-   @iyengar1988 discusses techniques to deal with this problem

## The Replication Crisis

-   The last Replication Crisis seems to have started in *Psychology*,
    but spread fast to other branches of science
-   @simmons2011 “proves” that you will get, or at least feel, younger
    by listening to “When I’m Sixty-Four” by The Beatles.
    -   @simmons2011 suggest a solution to the problem
        -   list of requirements for authers (6) and guidelines for
            editors (4)

## The Replication Crisis

-   *Why Most Published Research Findings Are False* @ioannidis2005
    -   “There is increasing concern that in modern research, false
        findings may be the majority or even the vast majority of
        published research claims \[6–8\]. However, this should not be
        surprising. It can be proven that most claimed research findings
        are false.”
-   *Again, and again, and again* @jasny2011
    -   How do we promote the publication of replicable data?
    -   Introduction to Science special section on *Data Replication &
        Reproducibility*
    -   *Promoting an open research culture* @nosek2015
        -   Author guidelines for journals could help to promote
            transparency, openness, and reproducibility

## Big replication studies in psychology

-   *Estimating the reproducibility of psychological science* @aac4716
    -   totalt 270 forskere involvert
    -   Tried to replicate 100 studies from the last 3 years taken from
        3 of the most respected journals in the field
    -   Results: Replication effects were half the magnitude of original
        effects
    -   Results: Ninety­seven percent of original studies had significant
        results (p​​\< .05). Thirty­six percent of replications had
        significant results
    -   Results: In sum, a large portion of replications did not
        reproduce evidence supporting the original results despite using
        high­powered designs and original materials when available.
-   *Many labs 2: Investigating variation in replicability across
    samples and settings* @klein2018. See @opensciencecollaboration2015
    for abstract.
    -   “We conducted preregistered replications of 28 classic and
        contemporary published findings, with protocols that were peer
        reviewed in advance, to examine variation in effect magnitudes
        across samples and settings.”
    -   “Using the conventional criterion of statistical significance (p
        \< .05), we found that 15 (54%) of the replications provided
        evidence of a statistically significant effect in the same
        direction as the original finding.”

## What about economics?

-   Good intentions!
    -   [@frisch1933]

![frich in econometrica](frisch33.png)

## What about economics?

-   Example of such a dataset [@ezekiel1933]

![data set](ezekiel1933.png)

## What about economics?

-   1960, big economic models
    -   difficult to move from one mainframe to another, see
        @dewald1986.
    -   only results published, replication/reproduction nearly
        impossible

## The JMCB project (1982)

-   1982 The Journal of Money, Credit and Banking Project (NSF
    sponsored)
    -   Replication (with submitted data sets) so in a modern sense more
        of an attempt at testing reproducibility
    -   Results: Managed full replication for 2 out of more than 70
    -   Results: Some more with nearly the original results
    -   Results: Many was impossible to replicate because of missing
        data, faulty computer programs, no documentation e.t.c.
    -   Many authers just ingored request for data and code
    -   “It would be embarrassing to reveal the findings of the
        *Project* save for our belief that the findings would be little
        different from any other major economics journal”

## “The Solution”

-   Code and data archives at the journals
-   American Economic Association data archive, [@zotero-381]
-   Did the solution work?
-   *Do economics journal archives promote replicable research?*
    [@mccullough2008]
-   **Abstract.** All the long-standing archives at economics journals
    do not facilitate the re-production of published results. The
    data-only archives at *Journal of Business and Economic Statistics*
    and *Economic Journal* fail in part because most authors do not
    contribute data. Results published in the *FRB St. Louis Review* can
    rarely be reproduced using the data+code in the journal archive.
-   The solution does not seem to work very well

## “Extension of the solution”

-   Research Objects, [@bechhofer2013]
    -   “Research Objects, semantically rich aggregations of resources
        that provide the “units of knowledge” which supply structure for
        delivery of information as Linked Data.”
    -   *DataCite - A global registration agency for research data*,
        @brase2009
        -   International co-operations to register data sets and give
            them a unique DOI (Document Object Indentifier)
    -   EU initiative @eu2015
        -   “This report provides an overview on access to and
            preservation of scientific information in the EU Member
            States as well as Norway and Turkey.”

## Another solution, “computable documents”

-   What if the code is an integral part of the article?
    -   What is to be sent to the Journal is one document containing the
        text **and**
    -   the code to read in the data **and**
    -   the code to calculate the different models **and**
    -   the code to test them **and**
    -   the code to report the results.
    -   What would be submitted would be, together with the data, a
        fully reproducible document
-   The main idea is often attributed to @knuth1992, although Knuth
    explicitly points out that it was not his idea [@knuth1984]
-   Knuths main goal seems to be to improve the quality of the program
    documentation (and indirectly the quality of the code) with WEB.
-   Noweb, @ramsey
    -   “noweb is designed to meet the needs of literate programmers
        while remaining as simple as possible. Its primary advantages
        are simplicity, extensibility, and
        language-independence—especially noticeable when compared with
        other literate-programming tools. noweb uses 5 control sequences
        to WEB's 27.”

## Another solution, “computable documents”

-   “In the mid 1980’s, researchers at our laboratory noticed that a few
    months after completing a project, the researchers were usually
    unable to reproduce their own computational work without
    considerable agony. In 1991, the concept of electronic documents
    solved this problem by making scientific computations
    reproducible.”[@schwab1995]
    -   unix `make`files
-   Connection between code and reproducibility. All code necessary to
    generate figures freely avaiable (WaveLab a Matlab (Matlab is
    proprietary software) library).[@buckheit1995]
-   “solutions for practical reproducible research systems”,
    [@fomel2009]
    -   Introduction to special issue of *Computing in Science &
        Engineering* ( Volume: 11 , Issue: 1 , Jan.-Feb. 2009 )

## Another solution, “computable documents”

-   “It is important, if not essential, to integrate the computations
    and code used in data analyses, methodological descriptions,
    simulations, and so on with the documents that describe and rely on
    them.” [@gentleman2007][^2]
-   “We introduce the concept of a **compendium as both a container for
    the different elements that make up the document and its
    computations** (i.e. text, code, data, ...), and as a means for
    distributing, managing and updating the collection.”
-   “**The step from disseminating analyses via a compendium to
    reproducible research is a small one.** By reproducible research, we
    mean research papers with accom- panying software tools that allow
    the reader to directly reproduce the results and employ the methods
    that are presented in the research paper.”
-   “a compendium has one, or more, **self-contained live documents that
    can be regenerated in absolute detail by others**, and that can
    often be used in contexts other than the author’s original work.”

## Another solution, “computable documents”

-   “We **define a dynamic document as an ordered composition of code
    chunks and text chunks** that describe and discuss a problem and its
    solution. The ordering of the chunks **need not be simply
    sequential** but can support rich and complex document structures.”
-   “The mechanism or system used to transform a dynamic document into
    some desired output or view will be called a **transformer**.”
-   In @gentleman2005 parts of @golub1999 is reproduced as a *computable
    compendium*.

## Implementations

-   Sweave, executable code (R or Splus) in LaTex documents,
    [@leisch2002]
-   Knitr, [@xie2014], [@xie2015] and [@xie2020]
    -   R code in LaTex documents (like Sweave), but also support for R
        Markdown [@allaire2020], [@tierney], [@xie2018] and [@riederer]
    -   Markdown @gruber, plain text with simple markup. Intended for
        Grubers web site.
    -   R Markdown, markdown with R code-chuncks
-   RStudio [@rstudioteam2020], IDE for R supporting R Markdown and
    knitr
-   R Notebook, document type supported by RStudio that implements a
    notebook interface, i.e. output is returned to the notebook when it
    is run
    -   Code should be run sequentially, but each code-chunck can be run
        individually
    -   Support LaTex code for mathematical typesetting
    -   pandoc, [@zotero-382], functions as the *transformer* (from
        @gentleman2007) and can transform a R Markdown document into
        many different formats.
    -   including MS Word, html and pdf (via LaTex)

## Implementations

-   Introductory books on R, [@lander2017] and Data Science,
    [@grolemund] often includes chapters on how to write dynamic
    computable documents in R Studio

## References

[^1]: Subcommittee on Replicability in Science Advisory Committee to the
    National Science Foundation Directorate for Social, Behavioural, and
    Economic Sciences

[^2]: Robert Gentleman and Ross Ihaka, “the fathers” of R,
    @rcoreteam2020
