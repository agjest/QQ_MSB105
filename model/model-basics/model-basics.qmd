---
title: "Model basics"
author: "ag"
format:
  revealjs:
        theme: simple
        footer: '[MSB105](/index.html)'
        reveal_options:
        code-fold: show
        incremental: true
        smaller: true
        scrollable: true
        slide-number: c/t
editor: visual
editor_options:
  markdown:
    wrap: 72
    canonical: true
    chunk_output_type: console
echo: true
eval: true
bibliography: ag_model_basics.bib
nocite: '@*'
---

```{r setup}
# get tidyverse to shut up
suppressPackageStartupMessages({
  library(tidyverse)
  library(modelr)
  options(na.action = na.warn)
})
```

```{r}
# create a bib file for the R packages
# used in this document
# Note! Needs to do a touch ag_model_basics.bib in terminal before first run
# else stops when bibliography: "ag_model_basics.bib" not found in YAML
knitr::write_bib(
  c(
    "tidyverse",
    "modelr"
    ),
  file = "ag_model_basics.bib"
  )
```

## Introduction

-   r4ds has somewhat different perspective than econometrics
    -   more concerned with making predictions
    -   econometrics more concerned with testing a theoretical model on
        data
    -   chp. 23 contains a lot of code meant to build intuition around
        linear models
    -   we will skip most of it
-   Calculate the estimated coefficients with some matrix algebra
    instead

## Linear model

-   The model is specified with a formula notation
    -   `y ~ x` represents the model
    -   $$y = a_1 + a_2 · x$$
    -   Fit the model on some simulated data from *modelr* package, sim1

. . .

```{r}
s1_mod <- lm(y ~ x, data = sim1)
# Pick out the coefficients
coef(s1_mod)
```

## Predictions

```{r, eval=FALSE}
g1 <- sim1 %>% 
ggplot(mapping = aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE)
g1
```

## Predictions

```{r, echo=FALSE}
g1 <- sim1 %>% 
ggplot(mapping = aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(formula = "y ~ x", method = "lm", se = FALSE)
g1
```

## Adding predictions to sim1

-   Estimated linear model $$y = 4.2208 + 2.0515 · x$$
-   What values of y does our model predict for our x values?

## Adding predictions to sim1

-   Estimated linear model $$y = 4.2208 + 2.0515 · x$$
-   What values of y does our model predict for our x values?

. . .

```{r, eval = FALSE}
sim1a <- sim1 %>% 
  add_predictions(model = s1_mod)

g1 + geom_point(data = sim1a, mapping = aes(x = x, y = pred), colour = "red", size = 3)
```

## Adding predictions to sim1 {.smaller}

```{r, echo=FALSE}
sim1a <- sim1 %>% 
  add_predictions(model = s1_mod)

g1 + geom_point(data = sim1a, mapping = aes(x = x, y = pred), colour = "red", size = 3)
```

## Adding residuals to sim1a

```{r}
sim1a <- sim1a %>% 
  add_residuals(s1_mod)
head(sim1a, n = 3)
```

## Plotting x-values against residuals; points

```{r, eval=FALSE}
sim1a %>% 
  ggplot(mapping = aes(x = x, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

## Plotting x-values against residuals; points

```{r, echo=FALSE}
sim1a %>% 
  ggplot(mapping = aes(x = x, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

## Plotting x-values against residuals; frequency

```{r, eval=FALSE}
sim1a %>% 
  ggplot(mapping = aes(x = resid)) +
  # experiment with binwidth = 1 and binwidth = 0.25
  geom_freqpoly(binwidth = 0.5)
```

## Plotting x-values against residuals; frequency

```{r, echo=FALSE}
sim1a %>% 
  ggplot(mapping = aes(x = resid)) +
  # experiment with binwidth = 1 and binwidth = 0.25
  geom_freqpoly(binwidth = 0.5)
```

## Matrix multiplication

$$
M= \begin{bmatrix} a_{1,1} & a_{1,2} \\ a_{2,1} & a_{2,2} \\ a_{3,1} & a_{3,2} \end{bmatrix} \quad
t(M) = \begin{bmatrix}a_{1,1} & a_{2,1} & a_{3,1}\\
a_{1,2} & a_{2,2} & a_{3,2}\end{bmatrix}\quad \boldsymbol{M'M}
$$

. . .

-   Row 1 in t(m) is to be multiplied pairwise with column 1 in M and
    summed. This sum gives the new value at place (1,1) in the resulting
    matrix.
-   Row 1 in t(m) is to be multiplied pairwise with column 2 in M and
    summed. This sum gives the new value at place (1,2) in the resulting
    matrix.
-   Row 2 in t(m) is to be multiplied pairwise with column 1 in M and
    summed. This sum gives the new value at place (2,1) in the resulting
    matrix.
-   Row 2 in t(m) is to be multiplied pairwise with column 2 in M and
    summed. This sum gives the new value at place (2,2) in the resulting
    matrix.

. . .

$$
t(M) * M = 
$$

$$\begin{bmatrix}a_{1,1}·a_{1,1} + a_{2,1} · a_{2,1} + a_{3,1} · a_{3,1} & a_{1,1}·a_{1,2} + a_{2,1} · a_{2,2} + a_{3,1} · a_{3,2} \\ a_{1,2}·a_{1,1} + a_{2,2} · a_{2,1} + a_{3,2} · a_{3,1} & a_{1,2}·a_{1,2} + a_{2,2} · a_{2,2} + a_{3,2} · a_{3,2}\end{bmatrix}$$

## Matrix multiplication

$$t(M) = \begin{bmatrix}1 & 9 & 8\\3 & 7 & 4\end{bmatrix} \quad M = \begin{bmatrix}1 & 3\\9 & 7\\8 & 4\end{bmatrix}$$

$$t(M) * M = \begin{bmatrix}1·1 + 9·9 + 8·8 & 1·3 + 9·7+8·4\\3·1+7·9+4·8 & 3·3+7·7+4·4\end{bmatrix} = \begin{bmatrix}146 & 98\\98 & 74 \end{bmatrix}$$

. . .

-   The rules for **X · Y**
    -   X, m rows, n columns
    -   Y, s rows, t columns
-   For **X · Y** to be possible:
    -   n = s and the dimension of the result will be
    -   `m x t`, i.e. m rows and t columns.

## Matrix multiplication

-   Matrix multiplication in R are done with `%*%`

. . .

```{r}
M <- matrix(c(1, 3, 9, 7, 8, 4), nrow = 3, ncol = 2, byrow = TRUE)
t(M) %*% M # 2 X 3 3 x 2, 3 3 ok, result 2 x 2
```

## Matrix multiplication

$$M * t(M)\quad 3 x 2\quad 2 x 3$$

. . .

**M \* t(M)** is possible since we have 2's in the middle. The dimension
of the result will be 3x3.

. . .

$$M * t(M) = 
\begin{bmatrix}1·1+3·3 & 1·9+3·7&1·8+3·4\\
9·1+7·3&9·9+7·7&9·8+7·4\\
8·1+4·3&8·9+4·7&8·8+4·4
\end{bmatrix}$$

## Matrix multiplication

$$\begin{bmatrix}
10 & 30 & 20 \\
30 & 130 & 100\\
20 & 100 & 80
\end{bmatrix}$$

```{r}
M %*% t(M)
```

## Multiplication by vector

-   Just like multiplying by a matrix with 1 row or 1 column

. . .

$$y = \begin{bmatrix}7\\2\\4\end{bmatrix}$$
$$(\boldsymbol{M} · t(\boldsymbol{M})) · \boldsymbol{y}$$

. . .

-   (M %\*% t(M)) is 3x3
-   **y** is 3x1
-   **(M · t(M)) · y**, 3x3 3x1
-   Possible, since 3x3 in the middle, and dimension of result will be
    3x1, 3 rows and 1 column

## Multiplication by vector

-   (M %\*% t(M)) is 3x3
-   **y** is 3x1
-   **(M · t(M)) · y**, 3x3 3x1
-   Possible, since 3x3 in the middle, and dimension of result will be
    3x1, 3 rows and 1 column

. . .

$$(M · t(M)) · y = \begin{bmatrix}
10 & 30 & 20 \\
30 & 130 & 100\\
20 & 100 & 80
\end{bmatrix} · \begin{bmatrix}7\\2\\4\end{bmatrix} = 
\begin{bmatrix}
10·7+30·2+20·4\\
30·7+130·2+100·4\\
20·7+100·2+80·4
\end{bmatrix}$$

$$=\begin{bmatrix}
210\\
870\\
660
\end{bmatrix}$$

## Multiplication by vector

```{r}
y <- matrix(c(7, 2, 4), nrow = 3)
y
```

```{r}
(M %*% t(M)) %*% y
```

## Model matrix

-   OLS: . . .

$$\hat{\beta} = (X^T X)^{-1} X^T y$$

-   X above is the model_matrix

. . .

```{r}
X_mod <- model_matrix(data = sim1a, formula = y ~ x)
X_mod <- X_mod %>% rename("a_1" = "(Intercept)")
dim(X_mod)
```

## Model matrix

```{r}
head(X_mod, n = 4)
tail(X_mod, n = 4)
```

## Model matrix {.smaller}

-   Use $$\hat{\beta} = (X^T X)^{-1} X^T y$$ to calculate the two
    coefficients

. . .

```{r}
X <- as.matrix(X_mod)
y <- sim1a$y
(XTX <- t(X) %*% X) # 2 x 30 * 30 x 2, result 2 x 2
(XTXinv <- solve(t(X) %*% X)) # invert
# check, should give I 2 x 2
round(XTX %*% XTXinv, 2)
```

## Model matrix {.smaller}

```{r}
M <- solve(t(X) %*% X) %*% t(X) # 2 x 2 * 2 x 30, result 2 x 30
round(M, 3)
```

```{r}
solve(t(X) %*% X) %*% t(X) %*% y # 2 x 30 * 30 x 1 result 2 x 1
```

. . .

-   The same coefficients we found earlier by using the `lm()` function.
-   Note! Finding the inverse of $X^T·X$, i.e. $(X^T·X)^{-1}$ is not
    trivial for larger models. The `lm()` uses many clever tricks to
    calculate it quickly while also avoiding numerical problems. So the
    calculations above are to show what's going on and should not be
    used on real models.

## Small dataset

-   Small dataset with 6 obs.
-   Makes it easier to see what's going on

. . .

```{r}
set.seed(444)
s <- seq_along(t(sim1$x))
sim1b <- sim1[sample(s,size = 6),]
sim1b
```

## Small dataset

```{r}
X_mod <- model_matrix(data = sim1b, formula = y ~ x)
X_mod <- X_mod %>% rename("a_1" = "(Intercept)")
X <- as.matrix(X_mod)
X
```

```{r}
round(solve(t(X) %*% X), 4)
```

## Small dataset

```{r}
round(solve(t(X) %*% X) %*% t(X), 4)
```

```{r}
(y <- as.matrix(sim1b$y, nrow = 6))
```

## Small dataset

```{r}
round(solve(t(X) %*% X) %*% t(X) %*% y, 4)
```

```{r}
coefficients(lm(y ~ x, data = sim1b))
```

## small dataset, no constant

-   In `lm()` we specifies that we do not want to estimate a constant
    with `lm(y ~ x - 1)`, i.e. by adding -1 to the formula.
-   This gives us the following model matrix

. . .

```{r}
X_mod <- model_matrix(data = sim1b, formula = y ~ x - 1)
X <- as.matrix(X_mod)
X
```

## small dataset, no constant

```{r}
round(solve(t(X) %*% X) %*% t(X), 4)
```

```{r}
round(solve(t(X) %*% X) %*% t(X) %*% y, 4)
```

```{r}
coefficients(lm(y ~ x - 1, data = sim1b))
```

## Small dataset, perfect multicolinearity {.smaller}

```{r}
sim1c <- sim1b %>% 
rename(x_1 = x) %>% 
mutate(x_2 = 2.25 * x_1)
```

```{r}
X_mod <- model_matrix(data = sim1c, formula = y ~ x_1 + x_2)
X <- as.matrix(X_mod)
X
```

```{r}
t(X) %*% X
```

## Small dataset, perfect multicolinearity

```{r, eval = FALSE}
solve(t(X) %*% X)
```

. . .

-   Result: Error in solve.default(t(X) %\*% X) : Lapack routine dgesv:
    system is exactly singular: U\[3,3\] = 0

-   The matrix don't have an inverse. The reason is that x_1 and x_2
    essentially contains the same information.

-   X has less than full rank.

## Small dataset, near multicolinearity, case 1

-   add small amount of white noise to x_3

. . .

```{r}
l = length(sim1c$x_2)
sim1c$x_3  = sim1c$x_2 + rnorm(n = l, mean = 0, sd = 0.001)
rm(l)
```

```{r}
X_mod <- model_matrix(data = sim1c, formula = y ~ x_1 + x_3)
X <- as.matrix(X_mod)
X
```

## Small dataset, near multicolinearity, case 1

```{r}
solve(t(X) %*% X)
```

```{r}
round(solve(t(X) %*% X) %*% t(X) %*% y, 4)
```

## Small dataset, near multicolinearity, case 2

-   add small amount of white noise to x_3

. . .

```{r}
l = length(sim1c$x_2)
sim1c$x_3  = sim1c$x_2 + rnorm(n = l, mean = 0, sd = 0.001)
rm(l)
```

```{r}
X_mod <- model_matrix(data = sim1c, formula = y ~ x_1 + x_3)
X <- as.matrix(X_mod)
X
```

## Small dataset, near multicolinearity, case 2

```{r}
solve(t(X) %*% X)
```

```{r}
round(solve(t(X) %*% X) %*% t(X) %*% y, 4)
```

## Conclusion case 1 and 2

-   Just tiny changes in x_3 gives dramatic changes in the coefficient
    estimates
-   Well known consequence of multicollinearity

## Small dataset, factor variable

```{r}
sim1c <- sim1c %>% 
mutate(x_4 = sample(c("M", "F"),
                    size = 6, 
                    prob = c(0.5, 0.5), 
                    replace = TRUE
                    )
       )
sim1c[1:3,]
```

## Small dataset, factor variable

```{r}
X_mod <- model_matrix(data = sim1c, formula = y ~ x_1 + x_4)
X <- as.matrix(X_mod)
X
```

## Small dataset, factor variable

```{r}
solve(t(X) %*% X)
```

```{r}
round(solve(t(X) %*% X) %*% t(X) %*% y, 4)
```

```{r}
coefficients(lm(y ~ x_1 + x_4, data = sim1c))
```

## Small dataset, interaction

-   formula `y ~ x_1 * x_4`
-   equation: y = a_0 + a_1 \* x_1 + a_4 \* x_4 + a_14 \* x_1 \* x_4

. . .

```{r}
X_mod <- model_matrix(data = sim1c, formula = y ~ x_1 * x_4)
X_mod
X <- as.matrix(X_mod)
```

## Small dataset, interaction

```{r}
solve(t(X) %*% X)
```

```{r}
beta <- solve(t(X) %*% X) %*% t(X) %*% y
round(beta, 4)
```

## Small dataset, interaction

```{r}
mod <- lm(y ~ x_1 * x_4, data = sim1c)
coefficients(mod)
sim1c <- sim1c %>% 
    select(y, x_1, x_2, x_4) %>% 
  add_predictions(mod)
```

```{r}
# Predicton for x_1 = 0 and x_1 = 10
x_0 <- c(1, 1, 1, 1); x_1 <- c(0, 10, 0, 10); x_4 <-  c(0, 0, 1, 1)
new_data <- matrix(c(x_0, x_1, x_4, x_1*x_4), ncol = 4)
new_data
```

## Small dataset, interaction {.smaller}

```{r}
new_pred <- new_data %*% beta
t(new_pred)
new_pred_data <- data.frame(x_1 = new_data[,2], x_4 = c("F", "F", "M", "M"), pred = new_pred)
new_pred_data
```

. . .

-   Plotting our interaction mode
-   Female:
    $y_F = 4.6195 + 1.9538· x_1 -1.7525 · x_4 + 0.3548 · x_1 · x_4$
    $= 4.6195 + 1.9538· x_1 -1.7525 · 0 + 0.3548 · x_1 · 0$
    $= 4.6195 + 1.9538· x_1$
-   Male:
    $y_M = 4.6195 + 1.9538· x_1 -1.7525 · x_4 + 0.3548 · x_1 · x_4$
    $= 4.6195 + 1.9538· x_1 -1.7525 · 1 + 0.3548 · x_1 · 1$
    $= 2.867 + 2.3086 · x_1$

## Small dataset, interaction

```{r, eval = FALSE}
ggplot(mapping = aes(x = x_1, y = y, colour = x_4), data = sim1c) +
  geom_point() +
  geom_point(mapping = aes(y = pred), size = 2) +
  geom_line(data = new_pred_data, mapping = aes(x = x_1, y = pred), lwd = 1)
```

## Small dataset, interaction

```{r, echo = FALSE}
ggplot(mapping = aes(x = x_1, y = y, colour = x_4), data = sim1c) +
  geom_point() +
  geom_point(mapping = aes(y = pred), size = 2) +
  geom_line(data = new_pred_data, mapping = aes(x = x_1, y = pred), lwd = 1)
```

## Transformations

-   Remember to put `+, *,^,-` into `I()` function
-   Else use transformations freely
-   No need to make a new transformed variable before using it in model

. . .

```{r, eval=FALSE}
# log transform x_2
mod2 <- lm(y ~ x_1 + log(x_2), data = sim1c)
sim1c %>% 
  ggplot(mapping = aes(x = x_1, y = log(x_2))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x')
```

## Transformations

```{r, echo=FALSE}
# log transform x_2
mod2 <- lm(y ~ x_1 + log(x_2), data = sim1c)
sim1c %>% 
  ggplot(mapping = aes(x = x_1, y = log(x_2))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = 'y ~ x')
```

## Missing values

-   Default R behaviour; drop missing values silently in models.
-   Can turn on warnings through setting an option.
    -   options(na.action = na.warn)
    -   will give warnings when observations are dropped
-   Get rid of warnings
    -   `na.action = na.exclude` as model function argument
    -   or reset option, options(na.action = na.exclude)

. . .

```{r}
options(na.action = na.warn)
options(na.action = na.exclude)
```

## References
